{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from nets.yolo4 import YoloBody\n",
    "from nets.yolo_training import YOLOLoss, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------#\n",
    "#   获得类和先验框\n",
    "#---------------------------------------------------#\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape([-1,3,2])[::-1,:,:]\n",
    "\n",
    "\n",
    "#---------------------------------------------------#\n",
    "#   训练一个epoch\n",
    "#---------------------------------------------------#\n",
    "def fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen,genval, Epoch, cuda, optimizer, lr_scheduler):\n",
    "    total_loss = 0\n",
    "    val_loss = 0\n",
    "    print('\\n' + '-' * 10 + 'Train one epoch.' + '-' * 10)\n",
    "    print('Epoch:'+ str(epoch+1) + '/' + str(Epoch))\n",
    "    print('Start Training.')\n",
    "    net.train()\n",
    "    for iteration in range(epoch_size):\n",
    "        start_time = time.time()\n",
    "        images, targets = next(gen)\n",
    "        with torch.no_grad():\n",
    "            if cuda:\n",
    "                images = Variable(torch.from_numpy(images).type(torch.FloatTensor)).cuda()\n",
    "                targets = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets]\n",
    "            else:\n",
    "                images = Variable(torch.from_numpy(images).type(torch.FloatTensor))\n",
    "                targets = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets]\n",
    "        optimizer.zero_grad()\n",
    "#         with torch.no_grad():\n",
    "#             outputs = net(images)\n",
    "        outputs = net(images)\n",
    "        losses = []\n",
    "        for i in range(3):\n",
    "            loss_item = yolo_losses[i](outputs[i], targets)\n",
    "            losses.append(loss_item[0])\n",
    "        loss = sum(losses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        waste_time = time.time() - start_time\n",
    "        if iteration == 0 or (iteration+1) % 10 == 0:\n",
    "            print('step:' + str(iteration+1) + '/' + str(epoch_size) + ' || Total Loss: %.4f || %.4fs/step' % (total_loss/(iteration+1), waste_time))\n",
    "    print('Finish Training.')\n",
    "    '''        \n",
    "    print('Start Validation.')\n",
    "    net.eval()\n",
    "    for iteration in range(epoch_size_val):\n",
    "        images_val, targets_val = next(genval)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if cuda:\n",
    "                images_val = Variable(torch.from_numpy(images_val).type(torch.FloatTensor)).cuda()\n",
    "                targets_val = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets_val]\n",
    "            else:\n",
    "                images_val = Variable(torch.from_numpy(images_val).type(torch.FloatTensor))\n",
    "                targets_val = [Variable(torch.from_numpy(ann).type(torch.FloatTensor)) for ann in targets_val]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images_val)\n",
    "            losses = []\n",
    "            for i in range(3):\n",
    "                loss_item = yolo_losses[i](outputs[i], targets_val)\n",
    "                losses.append(loss_item[0])\n",
    "            loss = sum(losses)\n",
    "            val_loss += loss\n",
    "    print('Finish Validation')\n",
    "    '''\n",
    "    print('Total Loss: %.4f || Val Loss: %.4f ' % (total_loss/(epoch_size+1), val_loss/(epoch_size_val+1)))\n",
    "    \n",
    "    return total_loss/(epoch_size+1), val_loss/(epoch_size_val+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "#   输入的shape大小\n",
    "#   显存比较小可以使用416x416\n",
    "#   显存比较大可以使用608x608\n",
    "#-------------------------------#\n",
    "# input_shape = (416,416)\n",
    "input_shape = (608, 608)\n",
    "\n",
    "#-------------------------------#\n",
    "#   tricks的使用设置\n",
    "#-------------------------------#\n",
    "Cosine_lr = True\n",
    "mosaic = True\n",
    "# 用于设定是否使用cuda\n",
    "Cuda = True\n",
    "smoooth_label = 0.03\n",
    "\n",
    "#-------------------------------#\n",
    "#   获得训练集和验证集的annotations\n",
    "#   \n",
    "#-------------------------------#\n",
    "train_annotation_path = 'model_data/mask_train.txt'\n",
    "val_annotation_path = 'model_data/mask_val.txt'\n",
    "\n",
    "#-------------------------------#\n",
    "#   获得先验框和类\n",
    "#-------------------------------#\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "classes_path = 'model_data/mask_classes.txt'   \n",
    "class_names = get_classes(classes_path)\n",
    "anchors = get_anchors(anchors_path)\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model weights.\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = YoloBody(len(anchors[0]), num_classes)\n",
    "#model_path = \"model_data/yolov4_coco_pretrained_weights.pth\"\n",
    "#model_path = \"model_data/yolov4_maskdetect_weights0.pth\"\n",
    "model_path = \"model_data/yolov4_maskdetect_weights1.pth\"\n",
    "# 加快模型训练的效率\n",
    "print('Loading pretrained model weights.')\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = torch.load(model_path)\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict[k]) ==  np.shape(v)}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "print('Finished!')\n",
    "\n",
    "if Cuda:\n",
    "    net = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    net = net.cuda()\n",
    "else:\n",
    "    net = torch.nn.DataParallel(model)\n",
    "\n",
    "# 建立loss函数\n",
    "yolo_losses = []\n",
    "for i in range(3):\n",
    "    yolo_losses.append(YOLOLoss(np.reshape(anchors, [-1,2]), num_classes, \\\n",
    "                                (input_shape[1], input_shape[0]), smoooth_label, Cuda))\n",
    "# read train lines and val lines\n",
    "with open(train_annotation_path) as f:\n",
    "    train_lines = f.readlines()\n",
    "with open(val_annotation_path) as f:\n",
    "    val_lines = f.readlines()\n",
    "num_train = len(train_lines)\n",
    "num_val = len(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:1/25\n",
      "Start Training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MaskDetect-YOLOv4-2\\nets\\yolo_training.py:504: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  tmp_targets = np.array(targets)\n",
      "d:\\python\\lib\\site-packages\\torch\\cuda\\nccl.py:14: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:1/321 || Total Loss: 13812.1582 || 10.9782s/step\n",
      "step:10/321 || Total Loss: 8798.8232 || 2.1696s/step\n",
      "step:20/321 || Total Loss: 6131.0508 || 2.2817s/step\n",
      "step:30/321 || Total Loss: 4634.3394 || 3.1193s/step\n",
      "step:40/321 || Total Loss: 3715.6829 || 3.0423s/step\n",
      "step:50/321 || Total Loss: 3103.4028 || 1.9164s/step\n",
      "step:60/321 || Total Loss: 2670.1426 || 2.9272s/step\n",
      "step:70/321 || Total Loss: 2347.0720 || 2.8882s/step\n",
      "step:80/321 || Total Loss: 2096.9109 || 3.3705s/step\n",
      "step:90/321 || Total Loss: 1896.5676 || 3.7778s/step\n",
      "step:100/321 || Total Loss: 1733.1326 || 3.5376s/step\n",
      "step:110/321 || Total Loss: 1596.3851 || 2.7440s/step\n",
      "step:120/321 || Total Loss: 1480.7101 || 2.8561s/step\n",
      "step:130/321 || Total Loss: 1381.4700 || 3.8309s/step\n",
      "step:140/321 || Total Loss: 1295.2689 || 3.1614s/step\n",
      "step:150/321 || Total Loss: 1219.4409 || 3.7638s/step\n",
      "step:160/321 || Total Loss: 1152.4675 || 3.0433s/step\n",
      "step:170/321 || Total Loss: 1092.5598 || 2.4738s/step\n",
      "step:180/321 || Total Loss: 1038.9232 || 2.6930s/step\n",
      "step:190/321 || Total Loss: 990.4301 || 2.2197s/step\n",
      "step:200/321 || Total Loss: 946.3223 || 3.1924s/step\n",
      "step:210/321 || Total Loss: 906.2298 || 1.7493s/step\n",
      "step:220/321 || Total Loss: 869.5374 || 1.7083s/step\n",
      "step:230/321 || Total Loss: 836.0730 || 3.0363s/step\n",
      "step:240/321 || Total Loss: 804.9825 || 3.1894s/step\n",
      "step:250/321 || Total Loss: 776.1700 || 2.9292s/step\n",
      "step:260/321 || Total Loss: 749.4739 || 2.2126s/step\n",
      "step:270/321 || Total Loss: 724.5697 || 2.2887s/step\n",
      "step:280/321 || Total Loss: 701.2573 || 1.8574s/step\n",
      "step:290/321 || Total Loss: 679.5197 || 3.5426s/step\n",
      "step:300/321 || Total Loss: 659.1329 || 3.1123s/step\n",
      "step:310/321 || Total Loss: 639.9984 || 2.4338s/step\n",
      "step:320/321 || Total Loss: 622.0084 || 1.5522s/step\n",
      "Finish Training.\n",
      "Total Loss: 618.3385 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:2/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 65.9385 || 2.6950s/step\n",
      "step:10/321 || Total Loss: 58.0350 || 2.1566s/step\n",
      "step:20/321 || Total Loss: 61.4845 || 4.1131s/step\n",
      "step:30/321 || Total Loss: 60.6170 || 2.5739s/step\n",
      "step:40/321 || Total Loss: 59.1294 || 1.7523s/step\n",
      "step:50/321 || Total Loss: 57.7074 || 2.1966s/step\n",
      "step:60/321 || Total Loss: 56.4851 || 2.7851s/step\n",
      "step:70/321 || Total Loss: 55.3452 || 2.6340s/step\n",
      "step:80/321 || Total Loss: 54.8819 || 1.5781s/step\n",
      "step:90/321 || Total Loss: 53.9353 || 2.0816s/step\n",
      "step:100/321 || Total Loss: 53.0680 || 4.1281s/step\n",
      "step:110/321 || Total Loss: 52.2378 || 3.9279s/step\n",
      "step:120/321 || Total Loss: 51.3423 || 2.3257s/step\n",
      "step:130/321 || Total Loss: 50.8018 || 2.8331s/step\n",
      "step:140/321 || Total Loss: 50.2466 || 3.5757s/step\n",
      "step:150/321 || Total Loss: 49.7938 || 3.4596s/step\n",
      "step:160/321 || Total Loss: 49.1871 || 2.1496s/step\n",
      "step:170/321 || Total Loss: 48.5734 || 3.3795s/step\n",
      "step:180/321 || Total Loss: 47.8899 || 3.5707s/step\n",
      "step:190/321 || Total Loss: 47.2839 || 2.4238s/step\n",
      "step:200/321 || Total Loss: 46.7679 || 3.2464s/step\n",
      "step:210/321 || Total Loss: 46.0949 || 3.3005s/step\n",
      "step:220/321 || Total Loss: 45.6271 || 2.4318s/step\n",
      "step:230/321 || Total Loss: 45.1267 || 3.5426s/step\n",
      "step:240/321 || Total Loss: 44.6289 || 2.8662s/step\n",
      "step:250/321 || Total Loss: 44.1982 || 2.2457s/step\n",
      "step:260/321 || Total Loss: 43.8017 || 2.8892s/step\n",
      "step:270/321 || Total Loss: 43.4870 || 3.5767s/step\n",
      "step:280/321 || Total Loss: 43.1388 || 2.5329s/step\n",
      "step:290/321 || Total Loss: 42.8671 || 3.6687s/step\n",
      "step:300/321 || Total Loss: 42.4800 || 3.0173s/step\n",
      "step:310/321 || Total Loss: 42.1336 || 2.4088s/step\n",
      "step:320/321 || Total Loss: 41.8990 || 3.3855s/step\n",
      "Finish Training.\n",
      "Total Loss: 41.7648 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:3/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 32.9254 || 2.1606s/step\n",
      "step:10/321 || Total Loss: 33.4400 || 2.8141s/step\n",
      "step:20/321 || Total Loss: 30.9676 || 1.9975s/step\n",
      "step:30/321 || Total Loss: 31.6909 || 2.5569s/step\n",
      "step:40/321 || Total Loss: 31.3165 || 2.1826s/step\n",
      "step:50/321 || Total Loss: 30.4637 || 2.5489s/step\n",
      "step:60/321 || Total Loss: 30.2581 || 3.9199s/step\n",
      "step:70/321 || Total Loss: 30.4844 || 2.4749s/step\n",
      "step:80/321 || Total Loss: 29.8975 || 2.3708s/step\n",
      "step:90/321 || Total Loss: 29.7476 || 2.6610s/step\n",
      "step:100/321 || Total Loss: 29.2686 || 2.3878s/step\n",
      "step:110/321 || Total Loss: 29.0026 || 1.6252s/step\n",
      "step:120/321 || Total Loss: 28.7755 || 3.1313s/step\n",
      "step:130/321 || Total Loss: 28.8390 || 2.3267s/step\n",
      "step:140/321 || Total Loss: 28.7783 || 3.3885s/step\n",
      "step:150/321 || Total Loss: 28.6542 || 2.6180s/step\n",
      "step:160/321 || Total Loss: 28.2890 || 3.0803s/step\n",
      "step:170/321 || Total Loss: 28.1644 || 2.5909s/step\n",
      "step:180/321 || Total Loss: 28.0742 || 4.5694s/step\n",
      "step:190/321 || Total Loss: 27.9242 || 2.4668s/step\n",
      "step:200/321 || Total Loss: 27.6583 || 2.5379s/step\n",
      "step:210/321 || Total Loss: 27.3939 || 1.8414s/step\n",
      "step:220/321 || Total Loss: 27.2537 || 3.8989s/step\n",
      "step:230/321 || Total Loss: 27.1502 || 2.7681s/step\n",
      "step:240/321 || Total Loss: 26.8962 || 3.1834s/step\n",
      "step:250/321 || Total Loss: 26.6625 || 1.8534s/step\n",
      "step:260/321 || Total Loss: 26.5572 || 2.0425s/step\n",
      "step:270/321 || Total Loss: 26.4930 || 2.3688s/step\n",
      "step:280/321 || Total Loss: 26.3189 || 3.8379s/step\n",
      "step:290/321 || Total Loss: 26.2606 || 2.0265s/step\n",
      "step:300/321 || Total Loss: 26.0451 || 3.0263s/step\n",
      "step:310/321 || Total Loss: 25.9029 || 2.7310s/step\n",
      "step:320/321 || Total Loss: 25.8251 || 1.7483s/step\n",
      "Finish Training.\n",
      "Total Loss: 25.7099 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:4/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 19.8582 || 2.8531s/step\n",
      "step:10/321 || Total Loss: 23.2808 || 3.0523s/step\n",
      "step:20/321 || Total Loss: 22.3215 || 1.8354s/step\n",
      "step:30/321 || Total Loss: 21.6058 || 2.9162s/step\n",
      "step:40/321 || Total Loss: 21.5388 || 3.4456s/step\n",
      "step:50/321 || Total Loss: 21.0538 || 2.8151s/step\n",
      "step:60/321 || Total Loss: 21.4103 || 4.1571s/step\n",
      "step:70/321 || Total Loss: 21.5165 || 3.2624s/step\n",
      "step:80/321 || Total Loss: 21.6437 || 2.8341s/step\n",
      "step:90/321 || Total Loss: 21.3174 || 2.1225s/step\n",
      "step:100/321 || Total Loss: 21.4047 || 2.7831s/step\n",
      "step:110/321 || Total Loss: 21.2120 || 3.1183s/step\n",
      "step:120/321 || Total Loss: 21.1134 || 2.3908s/step\n",
      "step:130/321 || Total Loss: 21.0818 || 3.2394s/step\n",
      "step:140/321 || Total Loss: 21.1404 || 3.4206s/step\n",
      "step:150/321 || Total Loss: 21.0363 || 2.8772s/step\n",
      "step:160/321 || Total Loss: 20.8571 || 2.5859s/step\n",
      "step:170/321 || Total Loss: 20.8608 || 3.3155s/step\n",
      "step:180/321 || Total Loss: 20.7732 || 3.1243s/step\n",
      "step:190/321 || Total Loss: 20.6464 || 2.6019s/step\n",
      "step:200/321 || Total Loss: 20.5821 || 4.1891s/step\n",
      "step:210/321 || Total Loss: 20.5695 || 1.5241s/step\n",
      "step:220/321 || Total Loss: 20.6440 || 3.2604s/step\n",
      "step:230/321 || Total Loss: 20.5296 || 1.8284s/step\n",
      "step:240/321 || Total Loss: 20.5310 || 3.2384s/step\n",
      "step:250/321 || Total Loss: 20.3812 || 3.3495s/step\n",
      "step:260/321 || Total Loss: 20.3043 || 2.7200s/step\n",
      "step:270/321 || Total Loss: 20.2584 || 2.9832s/step\n",
      "step:280/321 || Total Loss: 20.1676 || 2.6129s/step\n",
      "step:290/321 || Total Loss: 20.0357 || 2.6219s/step\n",
      "step:300/321 || Total Loss: 19.9548 || 3.7708s/step\n",
      "step:310/321 || Total Loss: 19.8260 || 2.2237s/step\n",
      "step:320/321 || Total Loss: 19.6564 || 3.6537s/step\n",
      "Finish Training.\n",
      "Total Loss: 19.5761 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:5/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 16.1893 || 3.2604s/step\n",
      "step:10/321 || Total Loss: 18.5880 || 3.7248s/step\n",
      "step:20/321 || Total Loss: 17.7400 || 2.3628s/step\n",
      "step:30/321 || Total Loss: 17.4188 || 2.4048s/step\n",
      "step:40/321 || Total Loss: 17.1896 || 2.7701s/step\n",
      "step:50/321 || Total Loss: 16.7753 || 2.3828s/step\n",
      "step:60/321 || Total Loss: 16.6844 || 1.9895s/step\n",
      "step:70/321 || Total Loss: 17.4749 || 2.4378s/step\n",
      "step:80/321 || Total Loss: 17.6999 || 2.1476s/step\n",
      "step:90/321 || Total Loss: 17.6701 || 3.1193s/step\n",
      "step:100/321 || Total Loss: 17.3905 || 2.4618s/step\n",
      "step:110/321 || Total Loss: 17.2877 || 3.1534s/step\n",
      "step:120/321 || Total Loss: 17.1375 || 3.0853s/step\n",
      "step:130/321 || Total Loss: 17.0988 || 3.2154s/step\n",
      "step:140/321 || Total Loss: 17.0709 || 4.1691s/step\n",
      "step:150/321 || Total Loss: 17.1100 || 3.6157s/step\n",
      "step:160/321 || Total Loss: 17.0576 || 2.1496s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:170/321 || Total Loss: 17.0081 || 3.6827s/step\n",
      "step:180/321 || Total Loss: 16.9212 || 3.4005s/step\n",
      "step:190/321 || Total Loss: 16.7686 || 2.8821s/step\n",
      "step:200/321 || Total Loss: 16.7258 || 3.0493s/step\n",
      "step:210/321 || Total Loss: 16.7310 || 2.8361s/step\n",
      "step:220/321 || Total Loss: 16.6782 || 3.8909s/step\n",
      "step:230/321 || Total Loss: 16.5930 || 3.0903s/step\n",
      "step:240/321 || Total Loss: 16.6162 || 3.0262s/step\n",
      "step:250/321 || Total Loss: 16.6641 || 2.8221s/step\n",
      "step:260/321 || Total Loss: 16.5904 || 2.8691s/step\n",
      "step:270/321 || Total Loss: 16.6944 || 2.8541s/step\n",
      "step:280/321 || Total Loss: 16.7056 || 3.0473s/step\n",
      "step:290/321 || Total Loss: 16.6438 || 3.1273s/step\n",
      "step:300/321 || Total Loss: 16.7014 || 3.6257s/step\n",
      "step:310/321 || Total Loss: 16.7036 || 3.4306s/step\n",
      "step:320/321 || Total Loss: 16.7213 || 1.6392s/step\n",
      "Finish Training.\n",
      "Total Loss: 16.6459 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:6/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 19.9619 || 2.8391s/step\n",
      "step:10/321 || Total Loss: 14.4664 || 2.1316s/step\n",
      "step:20/321 || Total Loss: 14.7951 || 3.5527s/step\n",
      "step:30/321 || Total Loss: 14.5079 || 3.0533s/step\n",
      "step:40/321 || Total Loss: 15.1631 || 2.7290s/step\n",
      "step:50/321 || Total Loss: 15.3366 || 1.7813s/step\n",
      "step:60/321 || Total Loss: 15.5312 || 3.0403s/step\n",
      "step:70/321 || Total Loss: 15.3392 || 2.3187s/step\n",
      "step:80/321 || Total Loss: 15.2818 || 3.1824s/step\n",
      "step:90/321 || Total Loss: 15.7407 || 2.3537s/step\n",
      "step:100/321 || Total Loss: 15.8436 || 3.7038s/step\n",
      "step:110/321 || Total Loss: 15.9959 || 2.6190s/step\n",
      "step:120/321 || Total Loss: 16.0283 || 2.5239s/step\n",
      "step:130/321 || Total Loss: 15.9824 || 2.5609s/step\n",
      "step:140/321 || Total Loss: 15.5605 || 1.8394s/step\n",
      "step:150/321 || Total Loss: 15.7262 || 3.7718s/step\n",
      "step:160/321 || Total Loss: 15.6563 || 2.7621s/step\n",
      "step:170/321 || Total Loss: 15.5118 || 2.4959s/step\n",
      "step:180/321 || Total Loss: 15.5384 || 2.3037s/step\n",
      "step:190/321 || Total Loss: 15.5422 || 1.9655s/step\n",
      "step:200/321 || Total Loss: 15.6738 || 3.1774s/step\n",
      "step:210/321 || Total Loss: 15.7235 || 2.2377s/step\n",
      "step:220/321 || Total Loss: 15.6603 || 3.9770s/step\n",
      "step:230/321 || Total Loss: 15.5840 || 3.4216s/step\n",
      "step:240/321 || Total Loss: 15.5415 || 2.7721s/step\n",
      "step:250/321 || Total Loss: 15.4248 || 3.5827s/step\n",
      "step:260/321 || Total Loss: 15.6039 || 4.4944s/step\n",
      "step:270/321 || Total Loss: 15.6569 || 2.7131s/step\n",
      "step:280/321 || Total Loss: 15.6391 || 3.6338s/step\n",
      "step:290/321 || Total Loss: 15.6732 || 3.6978s/step\n",
      "step:300/321 || Total Loss: 15.7109 || 2.2016s/step\n",
      "step:310/321 || Total Loss: 15.6120 || 1.9064s/step\n",
      "step:320/321 || Total Loss: 15.5757 || 1.9184s/step\n",
      "Finish Training.\n",
      "Total Loss: 15.5344 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:7/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 11.2558 || 2.8371s/step\n",
      "step:10/321 || Total Loss: 16.3022 || 2.7510s/step\n",
      "step:20/321 || Total Loss: 16.2656 || 2.1246s/step\n",
      "step:30/321 || Total Loss: 15.2835 || 2.8101s/step\n",
      "step:40/321 || Total Loss: 14.7241 || 2.6620s/step\n",
      "step:50/321 || Total Loss: 14.6315 || 3.0373s/step\n",
      "step:60/321 || Total Loss: 14.6765 || 3.7718s/step\n",
      "step:70/321 || Total Loss: 15.2372 || 1.8964s/step\n",
      "step:80/321 || Total Loss: 15.7230 || 3.6577s/step\n",
      "step:90/321 || Total Loss: 15.4590 || 2.5089s/step\n",
      "step:100/321 || Total Loss: 15.4675 || 3.4476s/step\n",
      "step:110/321 || Total Loss: 15.6139 || 2.7180s/step\n",
      "step:120/321 || Total Loss: 15.5688 || 2.8081s/step\n",
      "step:130/321 || Total Loss: 15.9031 || 1.9875s/step\n",
      "step:140/321 || Total Loss: 15.7580 || 1.8774s/step\n",
      "step:150/321 || Total Loss: 15.8540 || 3.9650s/step\n",
      "step:160/321 || Total Loss: 15.5774 || 3.6407s/step\n",
      "step:170/321 || Total Loss: 15.5058 || 3.0913s/step\n",
      "step:180/321 || Total Loss: 15.2345 || 3.6917s/step\n",
      "step:190/321 || Total Loss: 15.1262 || 2.4628s/step\n",
      "step:200/321 || Total Loss: 15.1157 || 2.8331s/step\n",
      "step:210/321 || Total Loss: 15.0032 || 2.8471s/step\n",
      "step:220/321 || Total Loss: 14.9067 || 2.0045s/step\n",
      "step:230/321 || Total Loss: 14.9067 || 2.5149s/step\n",
      "step:240/321 || Total Loss: 14.9959 || 3.1654s/step\n",
      "step:250/321 || Total Loss: 14.9236 || 2.0866s/step\n",
      "step:260/321 || Total Loss: 14.9540 || 2.9312s/step\n",
      "step:270/321 || Total Loss: 14.9841 || 3.2794s/step\n",
      "step:280/321 || Total Loss: 15.0278 || 2.8031s/step\n",
      "step:290/321 || Total Loss: 14.8932 || 1.8424s/step\n",
      "step:300/321 || Total Loss: 14.8195 || 2.8872s/step\n",
      "step:310/321 || Total Loss: 14.7993 || 3.5967s/step\n",
      "step:320/321 || Total Loss: 14.7565 || 3.2154s/step\n",
      "Finish Training.\n",
      "Total Loss: 14.7198 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:8/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 7.4064 || 3.0733s/step\n",
      "step:10/321 || Total Loss: 11.3094 || 2.8111s/step\n",
      "step:20/321 || Total Loss: 10.9719 || 3.5336s/step\n",
      "step:30/321 || Total Loss: 12.0500 || 3.3035s/step\n",
      "step:40/321 || Total Loss: 11.6064 || 3.5906s/step\n",
      "step:50/321 || Total Loss: 11.5799 || 2.6770s/step\n",
      "step:60/321 || Total Loss: 12.3764 || 2.8451s/step\n",
      "step:70/321 || Total Loss: 12.3457 || 2.4849s/step\n",
      "step:80/321 || Total Loss: 12.5352 || 2.3998s/step\n",
      "step:90/321 || Total Loss: 12.7712 || 2.9622s/step\n",
      "step:100/321 || Total Loss: 12.8941 || 3.3585s/step\n",
      "step:110/321 || Total Loss: 12.8275 || 2.6790s/step\n",
      "step:120/321 || Total Loss: 12.7738 || 2.9142s/step\n",
      "step:130/321 || Total Loss: 12.7919 || 1.7183s/step\n",
      "step:140/321 || Total Loss: 12.8478 || 2.3498s/step\n",
      "step:150/321 || Total Loss: 13.1237 || 3.0303s/step\n",
      "step:160/321 || Total Loss: 13.1293 || 4.5074s/step\n",
      "step:170/321 || Total Loss: 13.2097 || 3.0953s/step\n",
      "step:180/321 || Total Loss: 13.1757 || 3.0643s/step\n",
      "step:190/321 || Total Loss: 13.0749 || 3.5627s/step\n",
      "step:200/321 || Total Loss: 13.0253 || 2.9792s/step\n",
      "step:210/321 || Total Loss: 13.1389 || 3.5987s/step\n",
      "step:220/321 || Total Loss: 13.2612 || 3.2624s/step\n",
      "step:230/321 || Total Loss: 13.1431 || 3.9049s/step\n",
      "step:240/321 || Total Loss: 13.2195 || 3.0993s/step\n",
      "step:250/321 || Total Loss: 13.1504 || 3.3925s/step\n",
      "step:260/321 || Total Loss: 13.0749 || 2.4758s/step\n",
      "step:270/321 || Total Loss: 13.0090 || 2.5279s/step\n",
      "step:280/321 || Total Loss: 12.9349 || 2.4628s/step\n",
      "step:290/321 || Total Loss: 12.9040 || 3.2534s/step\n",
      "step:300/321 || Total Loss: 12.9239 || 2.8982s/step\n",
      "step:310/321 || Total Loss: 12.9473 || 3.5817s/step\n",
      "step:320/321 || Total Loss: 12.9609 || 2.4308s/step\n",
      "Finish Training.\n",
      "Total Loss: 12.9031 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:9/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 14.9173 || 3.5336s/step\n",
      "step:10/321 || Total Loss: 14.2211 || 3.0142s/step\n",
      "step:20/321 || Total Loss: 13.4135 || 2.3367s/step\n",
      "step:30/321 || Total Loss: 13.2444 || 3.1604s/step\n",
      "step:40/321 || Total Loss: 13.1930 || 2.6129s/step\n",
      "step:50/321 || Total Loss: 13.5770 || 3.5326s/step\n",
      "step:60/321 || Total Loss: 13.4678 || 2.4388s/step\n",
      "step:70/321 || Total Loss: 13.2313 || 3.3085s/step\n",
      "step:80/321 || Total Loss: 13.3021 || 3.7058s/step\n",
      "step:90/321 || Total Loss: 13.2013 || 2.8842s/step\n",
      "step:100/321 || Total Loss: 13.4019 || 3.3875s/step\n",
      "step:110/321 || Total Loss: 13.4747 || 3.4185s/step\n",
      "step:120/321 || Total Loss: 13.6174 || 2.8191s/step\n",
      "step:130/321 || Total Loss: 13.5443 || 3.5226s/step\n",
      "step:140/321 || Total Loss: 13.7719 || 3.2214s/step\n",
      "step:150/321 || Total Loss: 13.7570 || 1.6532s/step\n",
      "step:160/321 || Total Loss: 13.5294 || 4.4403s/step\n",
      "step:170/321 || Total Loss: 13.4871 || 2.7581s/step\n",
      "step:180/321 || Total Loss: 13.4330 || 2.5689s/step\n",
      "step:190/321 || Total Loss: 13.5044 || 2.8852s/step\n",
      "step:200/321 || Total Loss: 13.4143 || 3.4976s/step\n",
      "step:210/321 || Total Loss: 13.2627 || 2.9772s/step\n",
      "step:220/321 || Total Loss: 13.2079 || 3.5817s/step\n",
      "step:230/321 || Total Loss: 13.2770 || 2.0225s/step\n",
      "step:240/321 || Total Loss: 13.4851 || 3.1363s/step\n",
      "step:250/321 || Total Loss: 13.4185 || 2.6130s/step\n",
      "step:260/321 || Total Loss: 13.3916 || 2.6810s/step\n",
      "step:270/321 || Total Loss: 13.4621 || 2.6930s/step\n",
      "step:280/321 || Total Loss: 13.4078 || 2.0425s/step\n",
      "step:290/321 || Total Loss: 13.4203 || 2.9692s/step\n",
      "step:300/321 || Total Loss: 13.3165 || 3.0343s/step\n",
      "step:310/321 || Total Loss: 13.2670 || 3.1183s/step\n",
      "step:320/321 || Total Loss: 13.3620 || 4.2031s/step\n",
      "Finish Training.\n",
      "Total Loss: 13.3132 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:10/25\n",
      "Start Training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:1/321 || Total Loss: 6.9868 || 2.0916s/step\n",
      "step:10/321 || Total Loss: 12.6926 || 3.1554s/step\n",
      "step:20/321 || Total Loss: 12.6039 || 2.5279s/step\n",
      "step:30/321 || Total Loss: 12.5139 || 3.1173s/step\n",
      "step:40/321 || Total Loss: 13.1212 || 2.5799s/step\n",
      "step:50/321 || Total Loss: 13.4530 || 1.8314s/step\n",
      "step:60/321 || Total Loss: 13.3646 || 2.7631s/step\n",
      "step:70/321 || Total Loss: 12.9111 || 1.7613s/step\n",
      "step:80/321 || Total Loss: 12.8448 || 2.1076s/step\n",
      "step:90/321 || Total Loss: 12.4900 || 3.3495s/step\n",
      "step:100/321 || Total Loss: 12.2096 || 2.5129s/step\n",
      "step:110/321 || Total Loss: 12.5372 || 2.8081s/step\n",
      "step:120/321 || Total Loss: 12.2448 || 3.0072s/step\n",
      "step:130/321 || Total Loss: 12.0679 || 2.4899s/step\n",
      "step:140/321 || Total Loss: 12.5698 || 2.3548s/step\n",
      "step:150/321 || Total Loss: 12.6427 || 3.3805s/step\n",
      "step:160/321 || Total Loss: 12.7797 || 2.8882s/step\n",
      "step:170/321 || Total Loss: 12.9119 || 3.3125s/step\n",
      "step:180/321 || Total Loss: 12.8076 || 3.0483s/step\n",
      "step:190/321 || Total Loss: 12.9123 || 3.2384s/step\n",
      "step:200/321 || Total Loss: 12.8421 || 2.5418s/step\n",
      "step:210/321 || Total Loss: 12.8095 || 2.8731s/step\n",
      "step:220/321 || Total Loss: 12.7736 || 3.3075s/step\n",
      "step:230/321 || Total Loss: 12.6481 || 2.4178s/step\n",
      "step:240/321 || Total Loss: 12.6651 || 1.7753s/step\n",
      "step:250/321 || Total Loss: 12.7030 || 2.3347s/step\n",
      "step:260/321 || Total Loss: 12.7307 || 3.5777s/step\n",
      "step:270/321 || Total Loss: 12.6851 || 1.9304s/step\n",
      "step:280/321 || Total Loss: 12.6904 || 2.9242s/step\n",
      "step:290/321 || Total Loss: 12.7271 || 3.3325s/step\n",
      "step:300/321 || Total Loss: 12.6603 || 2.7240s/step\n",
      "step:310/321 || Total Loss: 12.5900 || 2.3928s/step\n",
      "step:320/321 || Total Loss: 12.7399 || 2.9612s/step\n",
      "Finish Training.\n",
      "Total Loss: 12.6897 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:11/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 9.8182 || 2.3948s/step\n",
      "step:10/321 || Total Loss: 9.9732 || 3.5326s/step\n",
      "step:20/321 || Total Loss: 10.7502 || 1.7653s/step\n",
      "step:30/321 || Total Loss: 10.7734 || 3.1744s/step\n",
      "step:40/321 || Total Loss: 11.1730 || 2.2657s/step\n",
      "step:50/321 || Total Loss: 11.2217 || 2.4588s/step\n",
      "step:60/321 || Total Loss: 11.3249 || 1.6342s/step\n",
      "step:70/321 || Total Loss: 11.6567 || 3.2254s/step\n",
      "step:80/321 || Total Loss: 11.3723 || 3.2604s/step\n",
      "step:90/321 || Total Loss: 11.1369 || 1.6843s/step\n",
      "step:100/321 || Total Loss: 11.1794 || 2.6049s/step\n",
      "step:110/321 || Total Loss: 11.2345 || 3.7338s/step\n",
      "step:120/321 || Total Loss: 11.3925 || 1.8003s/step\n",
      "step:130/321 || Total Loss: 11.4829 || 2.7761s/step\n",
      "step:140/321 || Total Loss: 11.6629 || 3.6908s/step\n",
      "step:150/321 || Total Loss: 11.7860 || 3.4806s/step\n",
      "step:160/321 || Total Loss: 11.7846 || 3.1343s/step\n",
      "step:170/321 || Total Loss: 11.8475 || 3.2114s/step\n",
      "step:180/321 || Total Loss: 11.9214 || 3.1854s/step\n",
      "step:190/321 || Total Loss: 12.0242 || 2.2927s/step\n",
      "step:200/321 || Total Loss: 12.0311 || 3.1363s/step\n",
      "step:210/321 || Total Loss: 12.0746 || 3.0032s/step\n",
      "step:220/321 || Total Loss: 12.1141 || 3.1453s/step\n",
      "step:230/321 || Total Loss: 11.9779 || 3.5256s/step\n",
      "step:240/321 || Total Loss: 11.9886 || 4.1351s/step\n",
      "step:250/321 || Total Loss: 12.0371 || 3.1734s/step\n",
      "step:260/321 || Total Loss: 11.9597 || 3.1213s/step\n",
      "step:270/321 || Total Loss: 11.9090 || 4.4423s/step\n",
      "step:280/321 || Total Loss: 11.9241 || 2.4568s/step\n",
      "step:290/321 || Total Loss: 11.9874 || 3.7198s/step\n",
      "step:300/321 || Total Loss: 11.9651 || 2.6039s/step\n",
      "step:310/321 || Total Loss: 11.9363 || 3.5166s/step\n",
      "step:320/321 || Total Loss: 11.9444 || 3.4536s/step\n",
      "Finish Training.\n",
      "Total Loss: 11.9090 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:12/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 15.2882 || 3.8448s/step\n",
      "step:10/321 || Total Loss: 13.3695 || 3.6017s/step\n",
      "step:20/321 || Total Loss: 14.0793 || 2.6480s/step\n",
      "step:30/321 || Total Loss: 14.9549 || 2.4208s/step\n",
      "step:40/321 || Total Loss: 13.9120 || 3.0022s/step\n",
      "step:50/321 || Total Loss: 13.6705 || 3.2845s/step\n",
      "step:60/321 || Total Loss: 13.5054 || 3.6647s/step\n",
      "step:70/321 || Total Loss: 13.7278 || 2.9862s/step\n",
      "step:80/321 || Total Loss: 13.3719 || 1.9004s/step\n",
      "step:90/321 || Total Loss: 13.2625 || 2.8872s/step\n",
      "step:100/321 || Total Loss: 13.1338 || 2.3427s/step\n",
      "step:110/321 || Total Loss: 13.3102 || 2.6220s/step\n",
      "step:120/321 || Total Loss: 13.2320 || 2.9192s/step\n",
      "step:130/321 || Total Loss: 12.9676 || 2.5749s/step\n",
      "step:140/321 || Total Loss: 12.6779 || 2.5259s/step\n",
      "step:150/321 || Total Loss: 12.5848 || 3.3065s/step\n",
      "step:160/321 || Total Loss: 12.4149 || 3.5166s/step\n",
      "step:170/321 || Total Loss: 12.5105 || 1.7743s/step\n",
      "step:180/321 || Total Loss: 12.4320 || 3.4646s/step\n",
      "step:190/321 || Total Loss: 12.4197 || 4.4253s/step\n",
      "step:200/321 || Total Loss: 12.4077 || 3.2344s/step\n",
      "step:210/321 || Total Loss: 12.4517 || 3.0933s/step\n",
      "step:220/321 || Total Loss: 12.4882 || 1.7182s/step\n",
      "step:230/321 || Total Loss: 12.4793 || 2.3027s/step\n",
      "step:240/321 || Total Loss: 12.4179 || 3.0413s/step\n",
      "step:250/321 || Total Loss: 12.4172 || 2.6510s/step\n",
      "step:260/321 || Total Loss: 12.4578 || 3.3915s/step\n",
      "step:270/321 || Total Loss: 12.4217 || 2.8251s/step\n",
      "step:280/321 || Total Loss: 12.2682 || 2.2727s/step\n",
      "step:290/321 || Total Loss: 12.2500 || 2.1586s/step\n",
      "step:300/321 || Total Loss: 12.3084 || 3.5516s/step\n",
      "step:310/321 || Total Loss: 12.2128 || 1.9725s/step\n",
      "step:320/321 || Total Loss: 12.1353 || 2.0095s/step\n",
      "Finish Training.\n",
      "Total Loss: 12.0919 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:13/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 19.3021 || 2.8761s/step\n",
      "step:10/321 || Total Loss: 11.7986 || 3.7168s/step\n",
      "step:20/321 || Total Loss: 11.6157 || 2.6290s/step\n",
      "step:30/321 || Total Loss: 11.5979 || 3.2204s/step\n",
      "step:40/321 || Total Loss: 11.7383 || 1.8674s/step\n",
      "step:50/321 || Total Loss: 11.7897 || 2.6009s/step\n",
      "step:60/321 || Total Loss: 11.8633 || 2.8031s/step\n",
      "step:70/321 || Total Loss: 11.9351 || 3.2224s/step\n",
      "step:80/321 || Total Loss: 11.7842 || 1.8984s/step\n",
      "step:90/321 || Total Loss: 11.5817 || 2.7781s/step\n",
      "step:100/321 || Total Loss: 11.7536 || 2.7310s/step\n",
      "step:110/321 || Total Loss: 11.6690 || 1.8334s/step\n",
      "step:120/321 || Total Loss: 11.5343 || 3.2314s/step\n",
      "step:130/321 || Total Loss: 11.5453 || 2.4839s/step\n",
      "step:140/321 || Total Loss: 11.4117 || 3.6127s/step\n",
      "step:150/321 || Total Loss: 11.4569 || 2.4689s/step\n",
      "step:160/321 || Total Loss: 11.5641 || 2.7020s/step\n",
      "step:170/321 || Total Loss: 11.6955 || 2.6930s/step\n",
      "step:180/321 || Total Loss: 11.7488 || 2.9932s/step\n",
      "step:190/321 || Total Loss: 11.7696 || 2.5469s/step\n",
      "step:200/321 || Total Loss: 11.7401 || 2.6700s/step\n",
      "step:210/321 || Total Loss: 11.7412 || 2.8051s/step\n",
      "step:220/321 || Total Loss: 11.8750 || 2.8341s/step\n",
      "step:230/321 || Total Loss: 11.8776 || 2.9472s/step\n",
      "step:240/321 || Total Loss: 11.9441 || 2.8411s/step\n",
      "step:250/321 || Total Loss: 11.9404 || 3.5136s/step\n",
      "step:260/321 || Total Loss: 11.8791 || 3.3145s/step\n",
      "step:270/321 || Total Loss: 11.8053 || 3.0343s/step\n",
      "step:280/321 || Total Loss: 11.7868 || 3.8859s/step\n",
      "step:290/321 || Total Loss: 11.7155 || 2.9782s/step\n",
      "step:300/321 || Total Loss: 11.9018 || 2.3788s/step\n",
      "step:310/321 || Total Loss: 11.8285 || 3.4105s/step\n",
      "step:320/321 || Total Loss: 11.8193 || 3.1953s/step\n",
      "Finish Training.\n",
      "Total Loss: 11.7919 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:14/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 11.7852 || 3.2584s/step\n",
      "step:10/321 || Total Loss: 9.4437 || 3.9339s/step\n",
      "step:20/321 || Total Loss: 9.4256 || 4.1581s/step\n",
      "step:30/321 || Total Loss: 10.6252 || 2.5329s/step\n",
      "step:40/321 || Total Loss: 10.5298 || 2.8341s/step\n",
      "step:50/321 || Total Loss: 10.2417 || 2.3647s/step\n",
      "step:60/321 || Total Loss: 11.3738 || 2.8271s/step\n",
      "step:70/321 || Total Loss: 11.4406 || 3.2955s/step\n",
      "step:80/321 || Total Loss: 11.6559 || 3.5327s/step\n",
      "step:90/321 || Total Loss: 11.4386 || 1.7303s/step\n",
      "step:100/321 || Total Loss: 11.3813 || 2.9492s/step\n",
      "step:110/321 || Total Loss: 11.3149 || 3.1463s/step\n",
      "step:120/321 || Total Loss: 11.2838 || 2.4088s/step\n",
      "step:130/321 || Total Loss: 11.6011 || 3.1393s/step\n",
      "step:140/321 || Total Loss: 11.4598 || 2.8852s/step\n",
      "step:150/321 || Total Loss: 11.6800 || 3.6878s/step\n",
      "step:160/321 || Total Loss: 11.6665 || 2.9692s/step\n",
      "step:170/321 || Total Loss: 11.7122 || 3.7958s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:180/321 || Total Loss: 11.6684 || 2.6570s/step\n",
      "step:190/321 || Total Loss: 11.6140 || 2.5689s/step\n",
      "step:200/321 || Total Loss: 11.5725 || 3.1073s/step\n",
      "step:210/321 || Total Loss: 11.4829 || 2.0535s/step\n",
      "step:220/321 || Total Loss: 11.4130 || 1.7763s/step\n",
      "step:230/321 || Total Loss: 11.4115 || 3.2164s/step\n",
      "step:240/321 || Total Loss: 11.4936 || 4.0210s/step\n",
      "step:250/321 || Total Loss: 11.4927 || 1.8684s/step\n",
      "step:260/321 || Total Loss: 11.4938 || 2.8651s/step\n",
      "step:270/321 || Total Loss: 11.4866 || 2.9412s/step\n",
      "step:280/321 || Total Loss: 11.5956 || 3.1744s/step\n",
      "step:290/321 || Total Loss: 11.6914 || 3.7338s/step\n",
      "step:300/321 || Total Loss: 11.6639 || 2.8581s/step\n",
      "step:310/321 || Total Loss: 11.5746 || 2.2557s/step\n",
      "step:320/321 || Total Loss: 11.5030 || 2.6510s/step\n",
      "Finish Training.\n",
      "Total Loss: 11.4501 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:15/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 15.6057 || 2.9942s/step\n",
      "step:10/321 || Total Loss: 10.6920 || 2.6630s/step\n",
      "step:20/321 || Total Loss: 13.2461 || 3.5817s/step\n",
      "step:30/321 || Total Loss: 13.1903 || 2.4969s/step\n",
      "step:40/321 || Total Loss: 12.4127 || 3.1694s/step\n",
      "step:50/321 || Total Loss: 12.6625 || 3.2334s/step\n",
      "step:60/321 || Total Loss: 12.2999 || 3.0052s/step\n",
      "step:70/321 || Total Loss: 12.0784 || 3.3695s/step\n",
      "step:80/321 || Total Loss: 11.9677 || 2.1846s/step\n",
      "step:90/321 || Total Loss: 11.8456 || 3.9429s/step\n",
      "step:100/321 || Total Loss: 11.8327 || 2.6480s/step\n",
      "step:110/321 || Total Loss: 11.9970 || 3.0603s/step\n",
      "step:120/321 || Total Loss: 12.0201 || 3.4075s/step\n",
      "step:130/321 || Total Loss: 12.0087 || 3.0783s/step\n",
      "step:140/321 || Total Loss: 11.9263 || 1.9495s/step\n",
      "step:150/321 || Total Loss: 11.7678 || 2.3528s/step\n",
      "step:160/321 || Total Loss: 11.9295 || 3.9570s/step\n",
      "step:170/321 || Total Loss: 11.7833 || 2.9052s/step\n",
      "step:180/321 || Total Loss: 11.7686 || 2.7130s/step\n",
      "step:190/321 || Total Loss: 11.8060 || 2.2147s/step\n",
      "step:200/321 || Total Loss: 11.8295 || 2.9622s/step\n",
      "step:210/321 || Total Loss: 11.7982 || 1.8424s/step\n",
      "step:220/321 || Total Loss: 11.8038 || 4.6475s/step\n",
      "step:230/321 || Total Loss: 11.8004 || 3.1333s/step\n",
      "step:240/321 || Total Loss: 11.8494 || 3.1213s/step\n",
      "step:250/321 || Total Loss: 11.7802 || 4.0190s/step\n",
      "step:260/321 || Total Loss: 11.8117 || 2.5319s/step\n",
      "step:270/321 || Total Loss: 11.7944 || 2.8531s/step\n",
      "step:280/321 || Total Loss: 11.8255 || 1.7193s/step\n",
      "step:290/321 || Total Loss: 11.9483 || 3.2985s/step\n",
      "step:300/321 || Total Loss: 11.9412 || 2.5529s/step\n",
      "step:310/321 || Total Loss: 11.8319 || 2.4128s/step\n",
      "step:320/321 || Total Loss: 11.7770 || 2.8021s/step\n",
      "Finish Training.\n",
      "Total Loss: 11.7225 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:16/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 7.3428 || 3.3805s/step\n",
      "step:10/321 || Total Loss: 8.6961 || 2.0866s/step\n",
      "step:20/321 || Total Loss: 8.9519 || 2.4018s/step\n",
      "step:30/321 || Total Loss: 9.8016 || 3.2234s/step\n",
      "step:40/321 || Total Loss: 9.8606 || 2.6260s/step\n",
      "step:50/321 || Total Loss: 10.4831 || 2.1546s/step\n",
      "step:60/321 || Total Loss: 10.4268 || 3.3165s/step\n",
      "step:70/321 || Total Loss: 10.4814 || 3.7478s/step\n",
      "step:80/321 || Total Loss: 10.3183 || 2.3578s/step\n",
      "step:90/321 || Total Loss: 10.4453 || 2.8651s/step\n",
      "step:100/321 || Total Loss: 10.3045 || 4.6565s/step\n",
      "step:110/321 || Total Loss: 10.0520 || 2.6110s/step\n",
      "step:120/321 || Total Loss: 10.0642 || 2.5369s/step\n",
      "step:130/321 || Total Loss: 9.9747 || 1.9835s/step\n",
      "step:140/321 || Total Loss: 9.7362 || 2.4879s/step\n",
      "step:150/321 || Total Loss: 9.8785 || 1.6863s/step\n",
      "step:160/321 || Total Loss: 9.8984 || 2.7070s/step\n",
      "step:170/321 || Total Loss: 9.8458 || 2.4889s/step\n",
      "step:180/321 || Total Loss: 9.9678 || 3.7778s/step\n",
      "step:190/321 || Total Loss: 10.0158 || 2.1676s/step\n",
      "step:200/321 || Total Loss: 10.0585 || 3.6398s/step\n",
      "step:210/321 || Total Loss: 10.1089 || 3.1253s/step\n",
      "step:220/321 || Total Loss: 10.0706 || 3.0213s/step\n",
      "step:230/321 || Total Loss: 10.2191 || 2.7931s/step\n",
      "step:240/321 || Total Loss: 10.2159 || 3.3075s/step\n",
      "step:250/321 || Total Loss: 10.2639 || 3.0473s/step\n",
      "step:260/321 || Total Loss: 10.2457 || 3.7188s/step\n",
      "step:270/321 || Total Loss: 10.2861 || 3.1924s/step\n",
      "step:280/321 || Total Loss: 10.2836 || 2.0315s/step\n",
      "step:290/321 || Total Loss: 10.3072 || 3.3135s/step\n",
      "step:300/321 || Total Loss: 10.2698 || 3.0022s/step\n",
      "step:310/321 || Total Loss: 10.1923 || 3.1353s/step\n",
      "step:320/321 || Total Loss: 10.2044 || 1.5962s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.1678 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:17/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 5.2736 || 3.2844s/step\n",
      "step:10/321 || Total Loss: 9.1418 || 3.5046s/step\n",
      "step:20/321 || Total Loss: 9.1213 || 2.2086s/step\n",
      "step:30/321 || Total Loss: 10.5020 || 3.3385s/step\n",
      "step:40/321 || Total Loss: 10.2408 || 2.8621s/step\n",
      "step:50/321 || Total Loss: 10.3557 || 3.5967s/step\n",
      "step:60/321 || Total Loss: 10.7027 || 2.2537s/step\n",
      "step:70/321 || Total Loss: 10.4367 || 3.2394s/step\n",
      "step:80/321 || Total Loss: 10.6124 || 2.7711s/step\n",
      "step:90/321 || Total Loss: 11.0600 || 2.6971s/step\n",
      "step:100/321 || Total Loss: 11.0444 || 2.1076s/step\n",
      "step:110/321 || Total Loss: 10.8828 || 2.5589s/step\n",
      "step:120/321 || Total Loss: 10.6572 || 3.4256s/step\n",
      "step:130/321 || Total Loss: 10.4712 || 2.7831s/step\n",
      "step:140/321 || Total Loss: 10.4617 || 1.8314s/step\n",
      "step:150/321 || Total Loss: 10.6379 || 3.7588s/step\n",
      "step:160/321 || Total Loss: 10.6735 || 2.9222s/step\n",
      "step:170/321 || Total Loss: 10.6137 || 2.8781s/step\n",
      "step:180/321 || Total Loss: 10.5429 || 3.4205s/step\n",
      "step:190/321 || Total Loss: 10.4032 || 2.9522s/step\n",
      "step:200/321 || Total Loss: 10.6219 || 3.6377s/step\n",
      "step:210/321 || Total Loss: 10.7321 || 3.9109s/step\n",
      "step:220/321 || Total Loss: 10.8332 || 2.6069s/step\n",
      "step:230/321 || Total Loss: 10.8966 || 1.6832s/step\n",
      "step:240/321 || Total Loss: 10.9081 || 1.7433s/step\n",
      "step:250/321 || Total Loss: 10.8835 || 2.8441s/step\n",
      "step:260/321 || Total Loss: 10.9870 || 2.2357s/step\n",
      "step:270/321 || Total Loss: 10.9672 || 3.3835s/step\n",
      "step:280/321 || Total Loss: 10.9077 || 2.5089s/step\n",
      "step:290/321 || Total Loss: 10.9480 || 2.9672s/step\n",
      "step:300/321 || Total Loss: 10.8473 || 2.5829s/step\n",
      "step:310/321 || Total Loss: 10.8366 || 3.6037s/step\n",
      "step:320/321 || Total Loss: 10.7685 || 3.4986s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.7378 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:18/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 5.3240 || 3.6197s/step\n",
      "step:10/321 || Total Loss: 8.9061 || 2.9132s/step\n",
      "step:20/321 || Total Loss: 10.4700 || 4.0861s/step\n",
      "step:30/321 || Total Loss: 10.3649 || 2.5669s/step\n",
      "step:40/321 || Total Loss: 10.8193 || 3.0723s/step\n",
      "step:50/321 || Total Loss: 10.5282 || 3.6197s/step\n",
      "step:60/321 || Total Loss: 10.1841 || 1.9755s/step\n",
      "step:70/321 || Total Loss: 10.9323 || 3.5907s/step\n",
      "step:80/321 || Total Loss: 10.7597 || 3.6787s/step\n",
      "step:90/321 || Total Loss: 10.7323 || 2.2587s/step\n",
      "step:100/321 || Total Loss: 10.9654 || 3.0012s/step\n",
      "step:110/321 || Total Loss: 11.0358 || 2.3608s/step\n",
      "step:120/321 || Total Loss: 10.7650 || 2.3978s/step\n",
      "step:130/321 || Total Loss: 10.7242 || 2.8832s/step\n",
      "step:140/321 || Total Loss: 10.6878 || 2.8791s/step\n",
      "step:150/321 || Total Loss: 10.7324 || 3.0323s/step\n",
      "step:160/321 || Total Loss: 10.9019 || 3.8469s/step\n",
      "step:170/321 || Total Loss: 10.9411 || 2.2757s/step\n",
      "step:180/321 || Total Loss: 10.8693 || 3.3495s/step\n",
      "step:190/321 || Total Loss: 10.7459 || 3.2744s/step\n",
      "step:200/321 || Total Loss: 10.8441 || 2.7641s/step\n",
      "step:210/321 || Total Loss: 10.8524 || 2.0966s/step\n",
      "step:220/321 || Total Loss: 10.8482 || 1.9935s/step\n",
      "step:230/321 || Total Loss: 10.8319 || 3.2094s/step\n",
      "step:240/321 || Total Loss: 10.8611 || 3.7448s/step\n",
      "step:250/321 || Total Loss: 10.7995 || 3.1033s/step\n",
      "step:260/321 || Total Loss: 10.7606 || 3.0203s/step\n",
      "step:270/321 || Total Loss: 10.7488 || 3.7548s/step\n",
      "step:280/321 || Total Loss: 10.8039 || 3.0773s/step\n",
      "step:290/321 || Total Loss: 10.8473 || 2.6340s/step\n",
      "step:300/321 || Total Loss: 10.8241 || 3.4686s/step\n",
      "step:310/321 || Total Loss: 10.8744 || 2.6610s/step\n",
      "step:320/321 || Total Loss: 10.8315 || 3.1554s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.7885 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:19/25\n",
      "Start Training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:1/321 || Total Loss: 6.1416 || 3.3495s/step\n",
      "step:10/321 || Total Loss: 10.7718 || 2.2777s/step\n",
      "step:20/321 || Total Loss: 12.8412 || 2.7901s/step\n",
      "step:30/321 || Total Loss: 12.1919 || 2.5479s/step\n",
      "step:40/321 || Total Loss: 12.1332 || 2.4708s/step\n",
      "step:50/321 || Total Loss: 11.2104 || 2.5009s/step\n",
      "step:60/321 || Total Loss: 11.0187 || 2.8611s/step\n",
      "step:70/321 || Total Loss: 10.7646 || 3.2044s/step\n",
      "step:80/321 || Total Loss: 10.9724 || 1.7703s/step\n",
      "step:90/321 || Total Loss: 11.2066 || 3.0773s/step\n",
      "step:100/321 || Total Loss: 10.9300 || 3.3665s/step\n",
      "step:110/321 || Total Loss: 11.0364 || 2.7601s/step\n",
      "step:120/321 || Total Loss: 10.7742 || 2.5659s/step\n",
      "step:130/321 || Total Loss: 10.5598 || 3.3765s/step\n",
      "step:140/321 || Total Loss: 10.8116 || 3.0152s/step\n",
      "step:150/321 || Total Loss: 10.8561 || 2.5709s/step\n",
      "step:160/321 || Total Loss: 10.8326 || 2.5559s/step\n",
      "step:170/321 || Total Loss: 10.7897 || 3.1213s/step\n",
      "step:180/321 || Total Loss: 10.7765 || 2.9152s/step\n",
      "step:190/321 || Total Loss: 10.8628 || 2.9822s/step\n",
      "step:200/321 || Total Loss: 10.8780 || 3.5847s/step\n",
      "step:210/321 || Total Loss: 11.0153 || 3.3175s/step\n",
      "step:220/321 || Total Loss: 11.0006 || 3.5777s/step\n",
      "step:230/321 || Total Loss: 10.9717 || 3.6127s/step\n",
      "step:240/321 || Total Loss: 10.9812 || 2.2987s/step\n",
      "step:250/321 || Total Loss: 10.9554 || 1.6202s/step\n",
      "step:260/321 || Total Loss: 10.9534 || 2.0435s/step\n",
      "step:270/321 || Total Loss: 10.8125 || 2.9342s/step\n",
      "step:280/321 || Total Loss: 10.8821 || 2.3678s/step\n",
      "step:290/321 || Total Loss: 10.8803 || 3.2324s/step\n",
      "step:300/321 || Total Loss: 10.9801 || 2.6720s/step\n",
      "step:310/321 || Total Loss: 11.0127 || 3.5797s/step\n",
      "step:320/321 || Total Loss: 11.0064 || 3.7298s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.9644 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:20/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 6.3935 || 2.2957s/step\n",
      "step:10/321 || Total Loss: 10.9756 || 3.2634s/step\n",
      "step:20/321 || Total Loss: 10.6506 || 3.2574s/step\n",
      "step:30/321 || Total Loss: 9.6547 || 4.4693s/step\n",
      "step:40/321 || Total Loss: 10.3674 || 2.0055s/step\n",
      "step:50/321 || Total Loss: 10.4827 || 3.7658s/step\n",
      "step:60/321 || Total Loss: 10.5547 || 2.6950s/step\n",
      "step:70/321 || Total Loss: 10.4541 || 3.3525s/step\n",
      "step:80/321 || Total Loss: 10.4664 || 1.9374s/step\n",
      "step:90/321 || Total Loss: 10.5617 || 2.7801s/step\n",
      "step:100/321 || Total Loss: 10.7073 || 3.5807s/step\n",
      "step:110/321 || Total Loss: 10.8991 || 2.4949s/step\n",
      "step:120/321 || Total Loss: 10.8314 || 2.3438s/step\n",
      "step:130/321 || Total Loss: 10.8973 || 2.8621s/step\n",
      "step:140/321 || Total Loss: 11.0330 || 3.7278s/step\n",
      "step:150/321 || Total Loss: 10.9477 || 2.0315s/step\n",
      "step:160/321 || Total Loss: 10.9358 || 2.4378s/step\n",
      "step:170/321 || Total Loss: 11.0267 || 2.0505s/step\n",
      "step:180/321 || Total Loss: 11.0119 || 3.3295s/step\n",
      "step:190/321 || Total Loss: 10.9406 || 3.2004s/step\n",
      "step:200/321 || Total Loss: 10.9701 || 2.0906s/step\n",
      "step:210/321 || Total Loss: 11.1205 || 3.2084s/step\n",
      "step:220/321 || Total Loss: 11.1161 || 2.0805s/step\n",
      "step:230/321 || Total Loss: 11.1024 || 2.1066s/step\n",
      "step:240/321 || Total Loss: 11.0056 || 2.4338s/step\n",
      "step:250/321 || Total Loss: 10.8481 || 3.9670s/step\n",
      "step:260/321 || Total Loss: 10.8585 || 2.7521s/step\n",
      "step:270/321 || Total Loss: 10.7735 || 3.6247s/step\n",
      "step:280/321 || Total Loss: 10.7124 || 2.7761s/step\n",
      "step:290/321 || Total Loss: 10.7788 || 2.8712s/step\n",
      "step:300/321 || Total Loss: 10.7108 || 2.5610s/step\n",
      "step:310/321 || Total Loss: 10.6333 || 3.0944s/step\n",
      "step:320/321 || Total Loss: 10.7588 || 2.4068s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.7150 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:21/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 11.1625 || 2.6630s/step\n",
      "step:10/321 || Total Loss: 13.3481 || 3.1834s/step\n",
      "step:20/321 || Total Loss: 11.6661 || 3.0913s/step\n",
      "step:30/321 || Total Loss: 11.7812 || 2.1716s/step\n",
      "step:40/321 || Total Loss: 11.7687 || 1.9164s/step\n",
      "step:50/321 || Total Loss: 11.3020 || 1.8474s/step\n",
      "step:60/321 || Total Loss: 11.6843 || 2.9762s/step\n",
      "step:70/321 || Total Loss: 11.3494 || 2.4849s/step\n",
      "step:80/321 || Total Loss: 11.0847 || 3.9499s/step\n",
      "step:90/321 || Total Loss: 11.5702 || 3.4576s/step\n",
      "step:100/321 || Total Loss: 11.4411 || 3.1053s/step\n",
      "step:110/321 || Total Loss: 11.3472 || 2.3437s/step\n",
      "step:120/321 || Total Loss: 11.0904 || 2.9422s/step\n",
      "step:130/321 || Total Loss: 10.9193 || 3.7728s/step\n",
      "step:140/321 || Total Loss: 10.7575 || 3.7018s/step\n",
      "step:150/321 || Total Loss: 10.7786 || 3.2644s/step\n",
      "step:160/321 || Total Loss: 10.6918 || 4.0640s/step\n",
      "step:170/321 || Total Loss: 10.7580 || 2.6990s/step\n",
      "step:180/321 || Total Loss: 10.7566 || 3.2574s/step\n",
      "step:190/321 || Total Loss: 10.7795 || 2.3237s/step\n",
      "step:200/321 || Total Loss: 10.7100 || 4.5044s/step\n",
      "step:210/321 || Total Loss: 10.8134 || 1.8874s/step\n",
      "step:220/321 || Total Loss: 10.7839 || 3.7418s/step\n",
      "step:230/321 || Total Loss: 10.8181 || 3.3625s/step\n",
      "step:240/321 || Total Loss: 10.8072 || 1.7273s/step\n",
      "step:250/321 || Total Loss: 10.6797 || 2.4658s/step\n",
      "step:260/321 || Total Loss: 10.6025 || 4.0020s/step\n",
      "step:270/321 || Total Loss: 10.5555 || 2.3708s/step\n",
      "step:280/321 || Total Loss: 10.5096 || 2.1256s/step\n",
      "step:290/321 || Total Loss: 10.4144 || 3.1504s/step\n",
      "step:300/321 || Total Loss: 10.4103 || 1.9655s/step\n",
      "step:310/321 || Total Loss: 10.3948 || 2.4889s/step\n",
      "step:320/321 || Total Loss: 10.3266 || 3.7198s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.2767 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:22/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 9.0843 || 2.7501s/step\n",
      "step:10/321 || Total Loss: 10.0731 || 3.3125s/step\n",
      "step:20/321 || Total Loss: 10.2628 || 3.5567s/step\n",
      "step:30/321 || Total Loss: 9.7807 || 3.5066s/step\n",
      "step:40/321 || Total Loss: 9.9319 || 3.9980s/step\n",
      "step:50/321 || Total Loss: 10.2660 || 3.0052s/step\n",
      "step:60/321 || Total Loss: 10.0422 || 3.7958s/step\n",
      "step:70/321 || Total Loss: 9.9848 || 2.5218s/step\n",
      "step:80/321 || Total Loss: 10.3599 || 4.7225s/step\n",
      "step:90/321 || Total Loss: 10.3593 || 3.2404s/step\n",
      "step:100/321 || Total Loss: 10.4050 || 2.2757s/step\n",
      "step:110/321 || Total Loss: 10.1663 || 3.5146s/step\n",
      "step:120/321 || Total Loss: 10.1452 || 1.9304s/step\n",
      "step:130/321 || Total Loss: 10.2129 || 2.9492s/step\n",
      "step:140/321 || Total Loss: 10.1173 || 2.2637s/step\n",
      "step:150/321 || Total Loss: 10.0862 || 2.6450s/step\n",
      "step:160/321 || Total Loss: 9.9482 || 3.0173s/step\n",
      "step:170/321 || Total Loss: 10.1201 || 2.9982s/step\n",
      "step:180/321 || Total Loss: 9.9920 || 3.1043s/step\n",
      "step:190/321 || Total Loss: 9.9625 || 3.7198s/step\n",
      "step:200/321 || Total Loss: 9.9486 || 3.8519s/step\n",
      "step:210/321 || Total Loss: 9.9651 || 3.7368s/step\n",
      "step:220/321 || Total Loss: 9.9189 || 2.9412s/step\n",
      "step:230/321 || Total Loss: 10.0310 || 2.0735s/step\n",
      "step:240/321 || Total Loss: 10.0288 || 3.7308s/step\n",
      "step:250/321 || Total Loss: 10.0510 || 2.2207s/step\n",
      "step:260/321 || Total Loss: 10.1232 || 2.6900s/step\n",
      "step:270/321 || Total Loss: 10.2499 || 2.9632s/step\n",
      "step:280/321 || Total Loss: 10.2379 || 3.7528s/step\n",
      "step:290/321 || Total Loss: 10.2308 || 2.7911s/step\n",
      "step:300/321 || Total Loss: 10.2468 || 1.6422s/step\n",
      "step:310/321 || Total Loss: 10.1518 || 2.4318s/step\n",
      "step:320/321 || Total Loss: 10.1092 || 2.9812s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.0573 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:23/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 8.9606 || 3.2284s/step\n",
      "step:10/321 || Total Loss: 11.6646 || 3.1714s/step\n",
      "step:20/321 || Total Loss: 10.8670 || 2.3988s/step\n",
      "step:30/321 || Total Loss: 10.5932 || 2.7911s/step\n",
      "step:40/321 || Total Loss: 10.0283 || 2.7911s/step\n",
      "step:50/321 || Total Loss: 9.6828 || 2.8381s/step\n",
      "step:60/321 || Total Loss: 9.7932 || 3.2774s/step\n",
      "step:70/321 || Total Loss: 9.5986 || 2.1786s/step\n",
      "step:80/321 || Total Loss: 9.8771 || 2.1506s/step\n",
      "step:90/321 || Total Loss: 9.8811 || 4.4533s/step\n",
      "step:100/321 || Total Loss: 9.8602 || 2.4708s/step\n",
      "step:110/321 || Total Loss: 9.7517 || 3.4566s/step\n",
      "step:120/321 || Total Loss: 9.6822 || 1.7563s/step\n",
      "step:130/321 || Total Loss: 9.8291 || 2.6360s/step\n",
      "step:140/321 || Total Loss: 9.8047 || 3.2674s/step\n",
      "step:150/321 || Total Loss: 10.0948 || 2.5999s/step\n",
      "step:160/321 || Total Loss: 10.0337 || 4.4583s/step\n",
      "step:170/321 || Total Loss: 10.0548 || 3.8979s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:180/321 || Total Loss: 10.0650 || 2.2967s/step\n",
      "step:190/321 || Total Loss: 10.0411 || 1.9915s/step\n",
      "step:200/321 || Total Loss: 10.0814 || 2.8051s/step\n",
      "step:210/321 || Total Loss: 10.0092 || 3.0653s/step\n",
      "step:220/321 || Total Loss: 10.0525 || 3.1054s/step\n",
      "step:230/321 || Total Loss: 10.1275 || 1.8073s/step\n",
      "step:240/321 || Total Loss: 10.3863 || 3.6637s/step\n",
      "step:250/321 || Total Loss: 10.3808 || 3.3635s/step\n",
      "step:260/321 || Total Loss: 10.3272 || 2.6049s/step\n",
      "step:270/321 || Total Loss: 10.3594 || 3.7628s/step\n",
      "step:280/321 || Total Loss: 10.3221 || 2.7991s/step\n",
      "step:290/321 || Total Loss: 10.3647 || 3.7959s/step\n",
      "step:300/321 || Total Loss: 10.3333 || 2.7721s/step\n",
      "step:310/321 || Total Loss: 10.2669 || 1.8384s/step\n",
      "step:320/321 || Total Loss: 10.1698 || 3.7348s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.1615 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:24/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 29.0811 || 4.0800s/step\n",
      "step:10/321 || Total Loss: 11.8689 || 2.6750s/step\n",
      "step:20/321 || Total Loss: 11.3875 || 3.5657s/step\n",
      "step:30/321 || Total Loss: 10.4295 || 2.9082s/step\n",
      "step:40/321 || Total Loss: 10.5228 || 2.4348s/step\n",
      "step:50/321 || Total Loss: 11.1768 || 2.6690s/step\n",
      "step:60/321 || Total Loss: 10.8472 || 4.0070s/step\n",
      "step:70/321 || Total Loss: 10.8068 || 3.5476s/step\n",
      "step:80/321 || Total Loss: 10.7099 || 2.5939s/step\n",
      "step:90/321 || Total Loss: 10.9758 || 1.7163s/step\n",
      "step:100/321 || Total Loss: 10.6542 || 2.5719s/step\n",
      "step:110/321 || Total Loss: 10.6207 || 3.1193s/step\n",
      "step:120/321 || Total Loss: 10.4115 || 2.9442s/step\n",
      "step:130/321 || Total Loss: 10.3821 || 2.2157s/step\n",
      "step:140/321 || Total Loss: 10.0916 || 1.7263s/step\n",
      "step:150/321 || Total Loss: 9.9586 || 2.2737s/step\n",
      "step:160/321 || Total Loss: 9.8112 || 3.0533s/step\n",
      "step:170/321 || Total Loss: 9.6399 || 2.8701s/step\n",
      "step:180/321 || Total Loss: 9.6722 || 1.9114s/step\n",
      "step:190/321 || Total Loss: 9.7344 || 3.0573s/step\n",
      "step:200/321 || Total Loss: 9.6346 || 3.3275s/step\n",
      "step:210/321 || Total Loss: 9.7272 || 2.4298s/step\n",
      "step:220/321 || Total Loss: 9.8197 || 1.8954s/step\n",
      "step:230/321 || Total Loss: 9.8855 || 3.0933s/step\n",
      "step:240/321 || Total Loss: 9.8774 || 2.7490s/step\n",
      "step:250/321 || Total Loss: 9.9329 || 2.6179s/step\n",
      "step:260/321 || Total Loss: 9.9562 || 2.2367s/step\n",
      "step:270/321 || Total Loss: 9.9753 || 3.4125s/step\n",
      "step:280/321 || Total Loss: 10.0891 || 2.4298s/step\n",
      "step:290/321 || Total Loss: 10.1622 || 3.0903s/step\n",
      "step:300/321 || Total Loss: 10.1503 || 2.5519s/step\n",
      "step:310/321 || Total Loss: 10.1239 || 3.2794s/step\n",
      "step:320/321 || Total Loss: 10.1401 || 3.3295s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.0947 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:25/25\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.9176 || 4.7385s/step\n",
      "step:10/321 || Total Loss: 9.0769 || 3.4716s/step\n",
      "step:20/321 || Total Loss: 9.0266 || 2.7701s/step\n",
      "step:30/321 || Total Loss: 9.4920 || 3.8749s/step\n",
      "step:40/321 || Total Loss: 9.9717 || 2.5969s/step\n",
      "step:50/321 || Total Loss: 9.8236 || 4.6285s/step\n",
      "step:60/321 || Total Loss: 10.0289 || 2.9282s/step\n",
      "step:70/321 || Total Loss: 10.1428 || 4.4373s/step\n",
      "step:80/321 || Total Loss: 10.3395 || 3.0403s/step\n",
      "step:90/321 || Total Loss: 10.8808 || 2.8471s/step\n",
      "step:100/321 || Total Loss: 11.0714 || 3.6357s/step\n",
      "step:110/321 || Total Loss: 10.8713 || 2.9032s/step\n",
      "step:120/321 || Total Loss: 10.8821 || 4.2051s/step\n",
      "step:130/321 || Total Loss: 10.8278 || 2.2076s/step\n",
      "step:140/321 || Total Loss: 10.7599 || 1.9054s/step\n",
      "step:150/321 || Total Loss: 10.8024 || 1.8394s/step\n",
      "step:160/321 || Total Loss: 10.7438 || 2.4468s/step\n",
      "step:170/321 || Total Loss: 10.6819 || 2.7220s/step\n",
      "step:180/321 || Total Loss: 10.6246 || 1.8884s/step\n",
      "step:190/321 || Total Loss: 10.5654 || 2.9662s/step\n",
      "step:200/321 || Total Loss: 10.5259 || 3.4716s/step\n",
      "step:210/321 || Total Loss: 10.5237 || 2.2987s/step\n",
      "step:220/321 || Total Loss: 10.5361 || 2.9682s/step\n",
      "step:230/321 || Total Loss: 10.4950 || 3.2614s/step\n",
      "step:240/321 || Total Loss: 10.4668 || 3.1834s/step\n",
      "step:250/321 || Total Loss: 10.5303 || 3.7298s/step\n",
      "step:260/321 || Total Loss: 10.4814 || 3.1393s/step\n",
      "step:270/321 || Total Loss: 10.4996 || 2.8922s/step\n",
      "step:280/321 || Total Loss: 10.3902 || 3.7308s/step\n",
      "step:290/321 || Total Loss: 10.2888 || 2.2537s/step\n",
      "step:300/321 || Total Loss: 10.3848 || 2.1186s/step\n",
      "step:310/321 || Total Loss: 10.3100 || 3.2965s/step\n",
      "step:320/321 || Total Loss: 10.2688 || 4.2272s/step\n",
      "Finish Training.\n",
      "Total Loss: 10.2268 || Val Loss: 0.0000 \n"
     ]
    }
   ],
   "source": [
    "#------------------------------------#\n",
    "#   先冻结backbone训练\n",
    "#------------------------------------#\n",
    "lr = 1e-3\n",
    "Batch_size = 4\n",
    "Init_Epoch = 0\n",
    "Freeze_Epoch = 25\n",
    "        \n",
    "optimizer = optim.Adam(net.parameters(), lr, weight_decay=5e-4)\n",
    "if Cosine_lr:\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)\n",
    "else:\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "gen = Generator(Batch_size, train_lines, (input_shape[0], input_shape[1])).generate(mosaic = mosaic)\n",
    "gen_val = Generator(Batch_size, val_lines, (input_shape[0], input_shape[1])).generate(mosaic = False)\n",
    "                        \n",
    "epoch_size = int(max(1, num_train//Batch_size//2.5)) if mosaic else max(1, num_train//Batch_size)\n",
    "epoch_size_val = num_val//Batch_size\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "best_loss = 99999999.0\n",
    "best_model_weights = copy.deepcopy(net.state_dict())\n",
    "for epoch in range(Init_Epoch, Freeze_Epoch):\n",
    "    total_loss, val_loss = fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, gen_val, \n",
    "                                         Freeze_Epoch, Cuda, optimizer, lr_scheduler)\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    with open('total_loss.csv', mode='a+') as total_loss_file:\n",
    "        total_loss_file.write(str(total_loss.item()) + '\\n')\n",
    "    #with open('val_loss.csv', mode='a+') as val_loss_file:\n",
    "    #    val_loss_file.write(str(val_loss.item()) + '\\n')\n",
    "torch.save(best_model_weights, 'model_data/yolov4_maskdetect_weights1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:26/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.5009 || 3.0973s/step\n",
      "step:10/321 || Total Loss: 8.8686 || 2.1456s/step\n",
      "step:20/321 || Total Loss: 7.8487 || 2.9942s/step\n",
      "step:30/321 || Total Loss: 8.6234 || 3.3725s/step\n",
      "step:40/321 || Total Loss: 9.2192 || 2.4058s/step\n",
      "step:50/321 || Total Loss: 9.0070 || 2.8531s/step\n",
      "step:60/321 || Total Loss: 8.9702 || 3.1994s/step\n",
      "step:70/321 || Total Loss: 8.8885 || 3.1143s/step\n",
      "step:80/321 || Total Loss: 8.9379 || 1.8133s/step\n",
      "step:90/321 || Total Loss: 8.8064 || 2.0115s/step\n",
      "step:100/321 || Total Loss: 9.0023 || 2.4338s/step\n",
      "step:110/321 || Total Loss: 9.0037 || 2.5659s/step\n",
      "step:120/321 || Total Loss: 8.9751 || 3.7698s/step\n",
      "step:130/321 || Total Loss: 9.0149 || 3.6988s/step\n",
      "step:140/321 || Total Loss: 8.9246 || 3.2784s/step\n",
      "step:150/321 || Total Loss: 9.0433 || 2.1286s/step\n",
      "step:160/321 || Total Loss: 8.9522 || 2.6520s/step\n",
      "step:170/321 || Total Loss: 8.9516 || 3.4266s/step\n",
      "step:180/321 || Total Loss: 8.7300 || 3.2314s/step\n",
      "step:190/321 || Total Loss: 8.6893 || 4.0060s/step\n",
      "step:200/321 || Total Loss: 8.5997 || 2.3878s/step\n",
      "step:210/321 || Total Loss: 8.6393 || 2.8872s/step\n",
      "step:220/321 || Total Loss: 8.5678 || 3.9009s/step\n",
      "step:230/321 || Total Loss: 8.5447 || 3.3035s/step\n",
      "step:240/321 || Total Loss: 8.5472 || 3.0002s/step\n",
      "step:250/321 || Total Loss: 8.5716 || 4.0170s/step\n",
      "step:260/321 || Total Loss: 8.5542 || 2.8131s/step\n",
      "step:270/321 || Total Loss: 8.5239 || 3.2734s/step\n",
      "step:280/321 || Total Loss: 8.4587 || 3.3225s/step\n",
      "step:290/321 || Total Loss: 8.4774 || 3.0563s/step\n",
      "step:300/321 || Total Loss: 8.4676 || 2.3137s/step\n",
      "step:310/321 || Total Loss: 8.5136 || 3.2794s/step\n",
      "step:320/321 || Total Loss: 8.4604 || 3.4246s/step\n",
      "Finish Training.\n",
      "Total Loss: 8.4320 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:27/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 6.7080 || 3.7568s/step\n",
      "step:10/321 || Total Loss: 8.7345 || 3.1964s/step\n",
      "step:20/321 || Total Loss: 7.6614 || 2.8321s/step\n",
      "step:30/321 || Total Loss: 7.4252 || 2.6109s/step\n",
      "step:40/321 || Total Loss: 7.7222 || 1.7913s/step\n",
      "step:50/321 || Total Loss: 8.0149 || 3.1243s/step\n",
      "step:60/321 || Total Loss: 7.8470 || 3.3205s/step\n",
      "step:70/321 || Total Loss: 7.5587 || 2.6840s/step\n",
      "step:80/321 || Total Loss: 7.8836 || 2.5169s/step\n",
      "step:90/321 || Total Loss: 7.6676 || 2.6560s/step\n",
      "step:100/321 || Total Loss: 7.6350 || 3.3625s/step\n",
      "step:110/321 || Total Loss: 7.6131 || 2.6170s/step\n",
      "step:120/321 || Total Loss: 7.6126 || 3.1273s/step\n",
      "step:130/321 || Total Loss: 7.5840 || 2.8892s/step\n",
      "step:140/321 || Total Loss: 7.4957 || 2.3097s/step\n",
      "step:150/321 || Total Loss: 7.4749 || 3.2614s/step\n",
      "step:160/321 || Total Loss: 7.5614 || 3.8369s/step\n",
      "step:170/321 || Total Loss: 7.4560 || 3.2454s/step\n",
      "step:180/321 || Total Loss: 7.4772 || 2.4879s/step\n",
      "step:190/321 || Total Loss: 7.6565 || 2.4789s/step\n",
      "step:200/321 || Total Loss: 7.8194 || 2.6190s/step\n",
      "step:210/321 || Total Loss: 7.7693 || 3.9700s/step\n",
      "step:220/321 || Total Loss: 7.8752 || 3.5587s/step\n",
      "step:230/321 || Total Loss: 7.8413 || 2.4718s/step\n",
      "step:240/321 || Total Loss: 7.8269 || 1.9244s/step\n",
      "step:250/321 || Total Loss: 7.8055 || 3.1794s/step\n",
      "step:260/321 || Total Loss: 7.7990 || 2.8721s/step\n",
      "step:270/321 || Total Loss: 7.7293 || 2.5769s/step\n",
      "step:280/321 || Total Loss: 7.8054 || 2.9972s/step\n",
      "step:290/321 || Total Loss: 7.7926 || 3.3685s/step\n",
      "step:300/321 || Total Loss: 7.7606 || 2.7100s/step\n",
      "step:310/321 || Total Loss: 7.6731 || 3.3646s/step\n",
      "step:320/321 || Total Loss: 7.7311 || 2.7561s/step\n",
      "Finish Training.\n",
      "Total Loss: 7.7006 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:28/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 9.2897 || 3.1293s/step\n",
      "step:10/321 || Total Loss: 8.4942 || 2.9812s/step\n",
      "step:20/321 || Total Loss: 8.4684 || 3.8319s/step\n",
      "step:30/321 || Total Loss: 8.2623 || 2.4628s/step\n",
      "step:40/321 || Total Loss: 7.8769 || 2.3147s/step\n",
      "step:50/321 || Total Loss: 8.1974 || 3.9860s/step\n",
      "step:60/321 || Total Loss: 8.8803 || 2.9382s/step\n",
      "step:70/321 || Total Loss: 8.2778 || 2.2467s/step\n",
      "step:80/321 || Total Loss: 8.3061 || 3.7868s/step\n",
      "step:90/321 || Total Loss: 8.4424 || 2.6740s/step\n",
      "step:100/321 || Total Loss: 8.7038 || 1.7803s/step\n",
      "step:110/321 || Total Loss: 8.8127 || 4.4123s/step\n",
      "step:120/321 || Total Loss: 8.6735 || 2.8391s/step\n",
      "step:130/321 || Total Loss: 8.4452 || 2.5379s/step\n",
      "step:140/321 || Total Loss: 8.4517 || 1.9444s/step\n",
      "step:150/321 || Total Loss: 8.3721 || 3.9499s/step\n",
      "step:160/321 || Total Loss: 8.2824 || 2.2727s/step\n",
      "step:170/321 || Total Loss: 8.3024 || 4.1881s/step\n",
      "step:180/321 || Total Loss: 8.3558 || 2.6310s/step\n",
      "step:190/321 || Total Loss: 8.3364 || 1.8194s/step\n",
      "step:200/321 || Total Loss: 8.3875 || 3.8519s/step\n",
      "step:210/321 || Total Loss: 8.3622 || 2.9202s/step\n",
      "step:220/321 || Total Loss: 8.3362 || 2.6330s/step\n",
      "step:230/321 || Total Loss: 8.2797 || 2.0986s/step\n",
      "step:240/321 || Total Loss: 8.2350 || 3.0042s/step\n",
      "step:250/321 || Total Loss: 8.2542 || 2.3708s/step\n",
      "step:260/321 || Total Loss: 8.2425 || 3.8549s/step\n",
      "step:270/321 || Total Loss: 8.2863 || 2.0886s/step\n",
      "step:280/321 || Total Loss: 8.2582 || 2.6980s/step\n",
      "step:290/321 || Total Loss: 8.1419 || 2.6890s/step\n",
      "step:300/321 || Total Loss: 8.1644 || 1.6642s/step\n",
      "step:310/321 || Total Loss: 8.1666 || 2.7360s/step\n",
      "step:320/321 || Total Loss: 8.1170 || 2.7390s/step\n",
      "Finish Training.\n",
      "Total Loss: 8.0887 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:29/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 8.7854 || 2.9672s/step\n",
      "step:10/321 || Total Loss: 6.5549 || 2.6500s/step\n",
      "step:20/321 || Total Loss: 6.1555 || 2.4128s/step\n",
      "step:30/321 || Total Loss: 6.4214 || 3.4856s/step\n",
      "step:40/321 || Total Loss: 6.6340 || 3.4536s/step\n",
      "step:50/321 || Total Loss: 7.1823 || 2.7741s/step\n",
      "step:60/321 || Total Loss: 7.1362 || 1.8634s/step\n",
      "step:70/321 || Total Loss: 7.0060 || 1.8654s/step\n",
      "step:80/321 || Total Loss: 6.8332 || 2.3548s/step\n",
      "step:90/321 || Total Loss: 7.0681 || 2.4108s/step\n",
      "step:100/321 || Total Loss: 7.0286 || 3.1343s/step\n",
      "step:110/321 || Total Loss: 7.1007 || 3.1584s/step\n",
      "step:120/321 || Total Loss: 7.1974 || 3.0353s/step\n",
      "step:130/321 || Total Loss: 7.2267 || 2.5259s/step\n",
      "step:140/321 || Total Loss: 7.2737 || 2.4718s/step\n",
      "step:150/321 || Total Loss: 7.1801 || 3.0833s/step\n",
      "step:160/321 || Total Loss: 7.1947 || 3.0082s/step\n",
      "step:170/321 || Total Loss: 7.1210 || 3.2084s/step\n",
      "step:180/321 || Total Loss: 7.0505 || 3.4015s/step\n",
      "step:190/321 || Total Loss: 7.1702 || 2.7931s/step\n",
      "step:200/321 || Total Loss: 7.3213 || 3.4826s/step\n",
      "step:210/321 || Total Loss: 7.4180 || 3.0753s/step\n",
      "step:220/321 || Total Loss: 7.3863 || 2.5759s/step\n",
      "step:230/321 || Total Loss: 7.4319 || 2.2417s/step\n",
      "step:240/321 || Total Loss: 7.3488 || 2.5719s/step\n",
      "step:250/321 || Total Loss: 7.2729 || 2.7340s/step\n",
      "step:260/321 || Total Loss: 7.2352 || 3.2544s/step\n",
      "step:270/321 || Total Loss: 7.1596 || 2.0765s/step\n",
      "step:280/321 || Total Loss: 7.1107 || 1.9775s/step\n",
      "step:290/321 || Total Loss: 7.0801 || 3.3045s/step\n",
      "step:300/321 || Total Loss: 7.0272 || 3.1544s/step\n",
      "step:310/321 || Total Loss: 6.9706 || 4.3122s/step\n",
      "step:320/321 || Total Loss: 6.9469 || 2.7561s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.9371 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:30/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 14.8872 || 2.8051s/step\n",
      "step:10/321 || Total Loss: 7.3620 || 2.9742s/step\n",
      "step:20/321 || Total Loss: 9.8464 || 4.0840s/step\n",
      "step:30/321 || Total Loss: 9.2385 || 2.8461s/step\n",
      "step:40/321 || Total Loss: 8.6153 || 2.4618s/step\n",
      "step:50/321 || Total Loss: 8.2124 || 2.9072s/step\n",
      "step:60/321 || Total Loss: 8.0208 || 2.9602s/step\n",
      "step:70/321 || Total Loss: 8.0639 || 3.9239s/step\n",
      "step:80/321 || Total Loss: 8.2240 || 2.2787s/step\n",
      "step:90/321 || Total Loss: 8.3557 || 2.8801s/step\n",
      "step:100/321 || Total Loss: 8.1532 || 3.4676s/step\n",
      "step:110/321 || Total Loss: 7.8872 || 2.6760s/step\n",
      "step:120/321 || Total Loss: 7.8851 || 2.8441s/step\n",
      "step:130/321 || Total Loss: 7.7653 || 2.5099s/step\n",
      "step:140/321 || Total Loss: 7.6331 || 2.9242s/step\n",
      "step:150/321 || Total Loss: 7.5831 || 2.7931s/step\n",
      "step:160/321 || Total Loss: 7.5507 || 3.0913s/step\n",
      "step:170/321 || Total Loss: 7.5263 || 2.6400s/step\n",
      "step:180/321 || Total Loss: 7.5329 || 3.6838s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:190/321 || Total Loss: 7.4267 || 2.6129s/step\n",
      "step:200/321 || Total Loss: 7.3420 || 3.3465s/step\n",
      "step:210/321 || Total Loss: 7.2832 || 3.9349s/step\n",
      "step:220/321 || Total Loss: 7.2857 || 1.8924s/step\n",
      "step:230/321 || Total Loss: 7.3081 || 3.2434s/step\n",
      "step:240/321 || Total Loss: 7.3087 || 2.9832s/step\n",
      "step:250/321 || Total Loss: 7.3722 || 2.5309s/step\n",
      "step:260/321 || Total Loss: 7.2809 || 2.7971s/step\n",
      "step:270/321 || Total Loss: 7.2006 || 3.7248s/step\n",
      "step:280/321 || Total Loss: 7.1491 || 3.1103s/step\n",
      "step:290/321 || Total Loss: 7.1558 || 3.1413s/step\n",
      "step:300/321 || Total Loss: 7.1591 || 2.5859s/step\n",
      "step:310/321 || Total Loss: 7.2538 || 2.6590s/step\n",
      "step:320/321 || Total Loss: 7.2301 || 2.2677s/step\n",
      "Finish Training.\n",
      "Total Loss: 7.2004 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:31/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.9189 || 2.4138s/step\n",
      "step:10/321 || Total Loss: 6.0950 || 3.1714s/step\n",
      "step:20/321 || Total Loss: 7.3387 || 3.4646s/step\n",
      "step:30/321 || Total Loss: 8.1565 || 2.6380s/step\n",
      "step:40/321 || Total Loss: 8.7239 || 3.6077s/step\n",
      "step:50/321 || Total Loss: 8.6333 || 2.6480s/step\n",
      "step:60/321 || Total Loss: 8.1430 || 2.4568s/step\n",
      "step:70/321 || Total Loss: 7.7474 || 3.0983s/step\n",
      "step:80/321 || Total Loss: 7.7643 || 3.7798s/step\n",
      "step:90/321 || Total Loss: 7.6680 || 3.5066s/step\n",
      "step:100/321 || Total Loss: 7.7200 || 2.9442s/step\n",
      "step:110/321 || Total Loss: 7.5061 || 2.9912s/step\n",
      "step:120/321 || Total Loss: 7.4955 || 2.4368s/step\n",
      "step:130/321 || Total Loss: 7.3278 || 2.1936s/step\n",
      "step:140/321 || Total Loss: 7.3468 || 1.9434s/step\n",
      "step:150/321 || Total Loss: 7.3612 || 3.9519s/step\n",
      "step:160/321 || Total Loss: 7.3653 || 2.1006s/step\n",
      "step:170/321 || Total Loss: 7.2950 || 2.9552s/step\n",
      "step:180/321 || Total Loss: 7.1705 || 1.9515s/step\n",
      "step:190/321 || Total Loss: 7.1219 || 3.4656s/step\n",
      "step:200/321 || Total Loss: 7.2087 || 3.6837s/step\n",
      "step:210/321 || Total Loss: 7.3427 || 3.3535s/step\n",
      "step:220/321 || Total Loss: 7.3994 || 2.2477s/step\n",
      "step:230/321 || Total Loss: 7.4298 || 2.8741s/step\n",
      "step:240/321 || Total Loss: 7.4420 || 2.9922s/step\n",
      "step:250/321 || Total Loss: 7.4085 || 2.8031s/step\n",
      "step:260/321 || Total Loss: 7.3219 || 3.3025s/step\n",
      "step:270/321 || Total Loss: 7.2615 || 2.6520s/step\n",
      "step:280/321 || Total Loss: 7.2678 || 2.9972s/step\n",
      "step:290/321 || Total Loss: 7.1887 || 3.1604s/step\n",
      "step:300/321 || Total Loss: 7.2163 || 2.6570s/step\n",
      "step:310/321 || Total Loss: 7.2563 || 3.4546s/step\n",
      "step:320/321 || Total Loss: 7.2262 || 2.2357s/step\n",
      "Finish Training.\n",
      "Total Loss: 7.2327 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:32/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.3478 || 2.7671s/step\n",
      "step:10/321 || Total Loss: 6.0655 || 4.4183s/step\n",
      "step:20/321 || Total Loss: 5.8640 || 3.1223s/step\n",
      "step:30/321 || Total Loss: 6.1863 || 3.0813s/step\n",
      "step:40/321 || Total Loss: 6.4629 || 2.8862s/step\n",
      "step:50/321 || Total Loss: 6.8935 || 1.9314s/step\n",
      "step:60/321 || Total Loss: 6.4447 || 3.1674s/step\n",
      "step:70/321 || Total Loss: 6.2327 || 2.7641s/step\n",
      "step:80/321 || Total Loss: 6.3541 || 2.5479s/step\n",
      "step:90/321 || Total Loss: 6.3719 || 3.0593s/step\n",
      "step:100/321 || Total Loss: 6.6573 || 2.8531s/step\n",
      "step:110/321 || Total Loss: 6.5106 || 2.2056s/step\n",
      "step:120/321 || Total Loss: 6.3555 || 3.3425s/step\n",
      "step:130/321 || Total Loss: 6.2854 || 2.8471s/step\n",
      "step:140/321 || Total Loss: 6.1890 || 2.6220s/step\n",
      "step:150/321 || Total Loss: 6.2734 || 1.6923s/step\n",
      "step:160/321 || Total Loss: 6.4695 || 3.7137s/step\n",
      "step:170/321 || Total Loss: 6.6626 || 2.6730s/step\n",
      "step:180/321 || Total Loss: 6.6750 || 2.8982s/step\n",
      "step:190/321 || Total Loss: 6.7309 || 3.3125s/step\n",
      "step:200/321 || Total Loss: 6.8074 || 2.4048s/step\n",
      "step:210/321 || Total Loss: 6.7496 || 1.8804s/step\n",
      "step:220/321 || Total Loss: 6.7590 || 3.6267s/step\n",
      "step:230/321 || Total Loss: 6.6933 || 3.1484s/step\n",
      "step:240/321 || Total Loss: 6.7461 || 2.7260s/step\n",
      "step:250/321 || Total Loss: 6.7773 || 3.6647s/step\n",
      "step:260/321 || Total Loss: 6.8214 || 1.7133s/step\n",
      "step:270/321 || Total Loss: 6.8240 || 3.4376s/step\n",
      "step:280/321 || Total Loss: 6.8635 || 3.3004s/step\n",
      "step:290/321 || Total Loss: 6.8417 || 2.3898s/step\n",
      "step:300/321 || Total Loss: 6.8334 || 2.0115s/step\n",
      "step:310/321 || Total Loss: 6.8528 || 3.0052s/step\n",
      "step:320/321 || Total Loss: 6.8556 || 2.4989s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.8280 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:33/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 7.6524 || 3.2384s/step\n",
      "step:10/321 || Total Loss: 5.4308 || 2.6950s/step\n",
      "step:20/321 || Total Loss: 6.0072 || 1.9374s/step\n",
      "step:30/321 || Total Loss: 5.9973 || 1.7783s/step\n",
      "step:40/321 || Total Loss: 6.3572 || 2.9993s/step\n",
      "step:50/321 || Total Loss: 6.5582 || 1.9094s/step\n",
      "step:60/321 || Total Loss: 6.8496 || 3.0723s/step\n",
      "step:70/321 || Total Loss: 6.7938 || 3.1443s/step\n",
      "step:80/321 || Total Loss: 6.8598 || 2.4639s/step\n",
      "step:90/321 || Total Loss: 6.6627 || 2.2497s/step\n",
      "step:100/321 || Total Loss: 6.8717 || 2.4849s/step\n",
      "step:110/321 || Total Loss: 6.9026 || 3.4526s/step\n",
      "step:120/321 || Total Loss: 6.7917 || 3.0113s/step\n",
      "step:130/321 || Total Loss: 7.1692 || 2.3728s/step\n",
      "step:140/321 || Total Loss: 7.2047 || 2.5629s/step\n",
      "step:150/321 || Total Loss: 7.1126 || 2.2997s/step\n",
      "step:160/321 || Total Loss: 7.0492 || 2.5719s/step\n",
      "step:170/321 || Total Loss: 6.9329 || 2.8531s/step\n",
      "step:180/321 || Total Loss: 6.9821 || 2.9412s/step\n",
      "step:190/321 || Total Loss: 7.0360 || 3.3005s/step\n",
      "step:200/321 || Total Loss: 7.0222 || 3.4045s/step\n",
      "step:210/321 || Total Loss: 6.9951 || 3.0533s/step\n",
      "step:220/321 || Total Loss: 6.9271 || 1.9715s/step\n",
      "step:230/321 || Total Loss: 7.0028 || 2.9672s/step\n",
      "step:240/321 || Total Loss: 7.1372 || 2.5489s/step\n",
      "step:250/321 || Total Loss: 7.1062 || 1.8633s/step\n",
      "step:260/321 || Total Loss: 7.1518 || 2.1786s/step\n",
      "step:270/321 || Total Loss: 7.1317 || 3.6017s/step\n",
      "step:280/321 || Total Loss: 7.0974 || 2.0385s/step\n",
      "step:290/321 || Total Loss: 7.0486 || 2.6109s/step\n",
      "step:300/321 || Total Loss: 6.9922 || 2.6950s/step\n",
      "step:310/321 || Total Loss: 7.0175 || 3.2374s/step\n",
      "step:320/321 || Total Loss: 7.0364 || 2.9292s/step\n",
      "Finish Training.\n",
      "Total Loss: 7.0174 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:34/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 2.4688 || 2.6390s/step\n",
      "step:10/321 || Total Loss: 6.1966 || 2.3818s/step\n",
      "step:20/321 || Total Loss: 5.5252 || 4.1321s/step\n",
      "step:30/321 || Total Loss: 6.1935 || 3.3605s/step\n",
      "step:40/321 || Total Loss: 6.5132 || 2.5509s/step\n",
      "step:50/321 || Total Loss: 6.6455 || 1.7703s/step\n",
      "step:60/321 || Total Loss: 6.4635 || 1.7863s/step\n",
      "step:70/321 || Total Loss: 6.7268 || 1.7143s/step\n",
      "step:80/321 || Total Loss: 6.4957 || 2.2927s/step\n",
      "step:90/321 || Total Loss: 6.5233 || 3.5206s/step\n",
      "step:100/321 || Total Loss: 6.5344 || 2.4939s/step\n",
      "step:110/321 || Total Loss: 6.6081 || 2.2957s/step\n",
      "step:120/321 || Total Loss: 6.8490 || 2.6180s/step\n",
      "step:130/321 || Total Loss: 6.8280 || 2.8851s/step\n",
      "step:140/321 || Total Loss: 6.8085 || 3.8359s/step\n",
      "step:150/321 || Total Loss: 6.6704 || 2.1356s/step\n",
      "step:160/321 || Total Loss: 6.7840 || 4.5504s/step\n",
      "step:170/321 || Total Loss: 6.6572 || 2.5909s/step\n",
      "step:180/321 || Total Loss: 6.8192 || 2.7040s/step\n",
      "step:190/321 || Total Loss: 6.8724 || 2.2787s/step\n",
      "step:200/321 || Total Loss: 6.9604 || 2.4378s/step\n",
      "step:210/321 || Total Loss: 6.8261 || 2.2867s/step\n",
      "step:220/321 || Total Loss: 6.8048 || 3.8869s/step\n",
      "step:230/321 || Total Loss: 6.8602 || 3.2454s/step\n",
      "step:240/321 || Total Loss: 6.8201 || 2.7410s/step\n",
      "step:250/321 || Total Loss: 6.9303 || 2.3718s/step\n",
      "step:260/321 || Total Loss: 6.9033 || 1.7533s/step\n",
      "step:270/321 || Total Loss: 6.9342 || 3.3455s/step\n",
      "step:280/321 || Total Loss: 6.8986 || 2.6490s/step\n",
      "step:290/321 || Total Loss: 6.9093 || 2.1716s/step\n",
      "step:300/321 || Total Loss: 6.9142 || 3.2614s/step\n",
      "step:310/321 || Total Loss: 6.8817 || 2.2927s/step\n",
      "step:320/321 || Total Loss: 6.8204 || 2.8441s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.8144 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:35/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 11.5005 || 3.9419s/step\n",
      "step:10/321 || Total Loss: 6.8164 || 3.2114s/step\n",
      "step:20/321 || Total Loss: 7.1926 || 3.6577s/step\n",
      "step:30/321 || Total Loss: 8.2738 || 2.9992s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:40/321 || Total Loss: 7.6673 || 2.6029s/step\n",
      "step:50/321 || Total Loss: 7.6937 || 1.7473s/step\n",
      "step:60/321 || Total Loss: 7.5916 || 3.7438s/step\n",
      "step:70/321 || Total Loss: 7.2869 || 3.9359s/step\n",
      "step:80/321 || Total Loss: 7.3217 || 3.0293s/step\n",
      "step:90/321 || Total Loss: 7.2205 || 3.2364s/step\n",
      "step:100/321 || Total Loss: 7.1230 || 3.4595s/step\n",
      "step:110/321 || Total Loss: 7.1247 || 3.1544s/step\n",
      "step:120/321 || Total Loss: 6.9635 || 2.3427s/step\n",
      "step:130/321 || Total Loss: 6.8479 || 4.0961s/step\n",
      "step:140/321 || Total Loss: 6.8882 || 2.2337s/step\n",
      "step:150/321 || Total Loss: 6.8574 || 3.9269s/step\n",
      "step:160/321 || Total Loss: 7.0396 || 1.9654s/step\n",
      "step:170/321 || Total Loss: 7.0179 || 3.9840s/step\n",
      "step:180/321 || Total Loss: 6.9958 || 3.1233s/step\n",
      "step:190/321 || Total Loss: 6.9924 || 2.2737s/step\n",
      "step:200/321 || Total Loss: 6.8955 || 2.9102s/step\n",
      "step:210/321 || Total Loss: 6.9054 || 2.7461s/step\n",
      "step:220/321 || Total Loss: 6.8798 || 2.8621s/step\n",
      "step:230/321 || Total Loss: 6.8800 || 2.9412s/step\n",
      "step:240/321 || Total Loss: 6.8538 || 2.2567s/step\n",
      "step:250/321 || Total Loss: 6.8229 || 2.6059s/step\n",
      "step:260/321 || Total Loss: 6.7656 || 2.9352s/step\n",
      "step:270/321 || Total Loss: 6.7680 || 3.0122s/step\n",
      "step:280/321 || Total Loss: 6.7412 || 2.1295s/step\n",
      "step:290/321 || Total Loss: 6.7852 || 1.7063s/step\n",
      "step:300/321 || Total Loss: 6.7879 || 2.2026s/step\n",
      "step:310/321 || Total Loss: 6.7825 || 2.8281s/step\n",
      "step:320/321 || Total Loss: 6.7612 || 2.0886s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.7411 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:36/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 12.6664 || 3.4515s/step\n",
      "step:10/321 || Total Loss: 6.2061 || 3.3676s/step\n",
      "step:20/321 || Total Loss: 5.7314 || 3.7008s/step\n",
      "step:30/321 || Total Loss: 5.7132 || 3.1674s/step\n",
      "step:40/321 || Total Loss: 6.1948 || 3.4326s/step\n",
      "step:50/321 || Total Loss: 6.6939 || 2.1976s/step\n",
      "step:60/321 || Total Loss: 6.7766 || 2.6130s/step\n",
      "step:70/321 || Total Loss: 6.6568 || 2.5929s/step\n",
      "step:80/321 || Total Loss: 6.6932 || 2.8201s/step\n",
      "step:90/321 || Total Loss: 6.6172 || 3.4286s/step\n",
      "step:100/321 || Total Loss: 6.5288 || 2.5759s/step\n",
      "step:110/321 || Total Loss: 6.3922 || 2.6380s/step\n",
      "step:120/321 || Total Loss: 6.4189 || 1.8844s/step\n",
      "step:130/321 || Total Loss: 6.5321 || 2.9382s/step\n",
      "step:140/321 || Total Loss: 6.5850 || 1.8093s/step\n",
      "step:150/321 || Total Loss: 6.6007 || 4.2872s/step\n",
      "step:160/321 || Total Loss: 6.5596 || 2.8511s/step\n",
      "step:170/321 || Total Loss: 6.5358 || 2.4298s/step\n",
      "step:180/321 || Total Loss: 6.5195 || 3.2024s/step\n",
      "step:190/321 || Total Loss: 6.6635 || 2.9812s/step\n",
      "step:200/321 || Total Loss: 6.7238 || 2.8531s/step\n",
      "step:210/321 || Total Loss: 6.6626 || 2.8721s/step\n",
      "step:220/321 || Total Loss: 6.7086 || 2.7170s/step\n",
      "step:230/321 || Total Loss: 6.6718 || 3.1774s/step\n",
      "step:240/321 || Total Loss: 6.7454 || 2.2647s/step\n",
      "step:250/321 || Total Loss: 6.6890 || 2.9072s/step\n",
      "step:260/321 || Total Loss: 6.6949 || 1.9324s/step\n",
      "step:270/321 || Total Loss: 6.7274 || 2.6450s/step\n",
      "step:280/321 || Total Loss: 6.8055 || 3.2955s/step\n",
      "step:290/321 || Total Loss: 6.7470 || 3.2624s/step\n",
      "step:300/321 || Total Loss: 6.7415 || 3.4226s/step\n",
      "step:310/321 || Total Loss: 6.8009 || 2.9522s/step\n",
      "step:320/321 || Total Loss: 6.8537 || 3.5827s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.8189 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:37/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.5359 || 2.4158s/step\n",
      "step:10/321 || Total Loss: 6.7298 || 3.3174s/step\n",
      "step:20/321 || Total Loss: 5.9828 || 2.4628s/step\n",
      "step:30/321 || Total Loss: 6.2134 || 3.4486s/step\n",
      "step:40/321 || Total Loss: 6.2210 || 3.1353s/step\n",
      "step:50/321 || Total Loss: 5.9770 || 2.7430s/step\n",
      "step:60/321 || Total Loss: 6.4163 || 2.6480s/step\n",
      "step:70/321 || Total Loss: 6.6070 || 2.7461s/step\n",
      "step:80/321 || Total Loss: 6.4578 || 3.2084s/step\n",
      "step:90/321 || Total Loss: 6.6293 || 3.3325s/step\n",
      "step:100/321 || Total Loss: 6.5087 || 3.1714s/step\n",
      "step:110/321 || Total Loss: 6.5340 || 2.9152s/step\n",
      "step:120/321 || Total Loss: 6.5082 || 3.2033s/step\n",
      "step:130/321 || Total Loss: 6.5502 || 2.2987s/step\n",
      "step:140/321 || Total Loss: 6.6276 || 3.5746s/step\n",
      "step:150/321 || Total Loss: 6.5720 || 3.2334s/step\n",
      "step:160/321 || Total Loss: 6.7199 || 3.0893s/step\n",
      "step:170/321 || Total Loss: 6.7713 || 1.9334s/step\n",
      "step:180/321 || Total Loss: 6.7116 || 2.5149s/step\n",
      "step:190/321 || Total Loss: 6.7339 || 3.6858s/step\n",
      "step:200/321 || Total Loss: 6.7250 || 2.3177s/step\n",
      "step:210/321 || Total Loss: 6.6292 || 3.7568s/step\n",
      "step:220/321 || Total Loss: 6.5991 || 4.0610s/step\n",
      "step:230/321 || Total Loss: 6.5024 || 2.6570s/step\n",
      "step:240/321 || Total Loss: 6.5906 || 2.2267s/step\n",
      "step:250/321 || Total Loss: 6.7280 || 2.4388s/step\n",
      "step:260/321 || Total Loss: 6.7384 || 3.5016s/step\n",
      "step:270/321 || Total Loss: 6.6961 || 1.5011s/step\n",
      "step:280/321 || Total Loss: 6.6898 || 2.4268s/step\n",
      "step:290/321 || Total Loss: 6.7539 || 2.1046s/step\n",
      "step:300/321 || Total Loss: 6.7535 || 2.9832s/step\n",
      "step:310/321 || Total Loss: 6.7854 || 3.6467s/step\n",
      "step:320/321 || Total Loss: 6.8061 || 3.4346s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.7775 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:38/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.2282 || 3.4916s/step\n",
      "step:10/321 || Total Loss: 4.7597 || 4.4793s/step\n",
      "step:20/321 || Total Loss: 6.0947 || 2.9042s/step\n",
      "step:30/321 || Total Loss: 5.7346 || 2.0465s/step\n",
      "step:40/321 || Total Loss: 5.6840 || 2.7451s/step\n",
      "step:50/321 || Total Loss: 5.6734 || 4.0561s/step\n",
      "step:60/321 || Total Loss: 6.3253 || 2.4198s/step\n",
      "step:70/321 || Total Loss: 6.4941 || 2.6740s/step\n",
      "step:80/321 || Total Loss: 6.6804 || 2.6039s/step\n",
      "step:90/321 || Total Loss: 6.4983 || 2.6410s/step\n",
      "step:100/321 || Total Loss: 6.6423 || 3.7238s/step\n",
      "step:110/321 || Total Loss: 6.7763 || 2.8631s/step\n",
      "step:120/321 || Total Loss: 6.6481 || 2.3327s/step\n",
      "step:130/321 || Total Loss: 6.6810 || 3.0373s/step\n",
      "step:140/321 || Total Loss: 6.5208 || 3.2835s/step\n",
      "step:150/321 || Total Loss: 6.6290 || 2.1356s/step\n",
      "step:160/321 || Total Loss: 6.6263 || 1.8374s/step\n",
      "step:170/321 || Total Loss: 6.7080 || 3.6167s/step\n",
      "step:180/321 || Total Loss: 6.6185 || 2.2647s/step\n",
      "step:190/321 || Total Loss: 6.8734 || 2.6650s/step\n",
      "step:200/321 || Total Loss: 6.7693 || 2.9272s/step\n",
      "step:210/321 || Total Loss: 6.8623 || 3.1824s/step\n",
      "step:220/321 || Total Loss: 6.8357 || 2.1616s/step\n",
      "step:230/321 || Total Loss: 6.8226 || 2.7200s/step\n",
      "step:240/321 || Total Loss: 6.6933 || 2.6890s/step\n",
      "step:250/321 || Total Loss: 6.6767 || 2.4808s/step\n",
      "step:260/321 || Total Loss: 6.6568 || 2.0055s/step\n",
      "step:270/321 || Total Loss: 6.6338 || 2.4498s/step\n",
      "step:280/321 || Total Loss: 6.6430 || 2.1446s/step\n",
      "step:290/321 || Total Loss: 6.6210 || 2.3888s/step\n",
      "step:300/321 || Total Loss: 6.6861 || 3.3625s/step\n",
      "step:310/321 || Total Loss: 6.6745 || 2.8101s/step\n",
      "step:320/321 || Total Loss: 6.7877 || 2.9232s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.7786 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:39/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 4.6335 || 2.8131s/step\n",
      "step:10/321 || Total Loss: 7.8539 || 2.9852s/step\n",
      "step:20/321 || Total Loss: 7.4444 || 3.5176s/step\n",
      "step:30/321 || Total Loss: 6.4535 || 3.7388s/step\n",
      "step:40/321 || Total Loss: 5.8577 || 2.5879s/step\n",
      "step:50/321 || Total Loss: 5.7411 || 2.8501s/step\n",
      "step:60/321 || Total Loss: 5.7308 || 2.5789s/step\n",
      "step:70/321 || Total Loss: 5.7319 || 1.8824s/step\n",
      "step:80/321 || Total Loss: 5.9391 || 3.9600s/step\n",
      "step:90/321 || Total Loss: 6.0951 || 2.8942s/step\n",
      "step:100/321 || Total Loss: 6.1327 || 2.5989s/step\n",
      "step:110/321 || Total Loss: 6.1500 || 3.1674s/step\n",
      "step:120/321 || Total Loss: 6.1981 || 1.6522s/step\n",
      "step:130/321 || Total Loss: 6.1595 || 3.4996s/step\n",
      "step:140/321 || Total Loss: 6.3456 || 1.9565s/step\n",
      "step:150/321 || Total Loss: 6.3148 || 3.8589s/step\n",
      "step:160/321 || Total Loss: 6.3542 || 3.4256s/step\n",
      "step:170/321 || Total Loss: 6.2775 || 2.1536s/step\n",
      "step:180/321 || Total Loss: 6.2477 || 2.8571s/step\n",
      "step:190/321 || Total Loss: 6.1843 || 2.8952s/step\n",
      "step:200/321 || Total Loss: 6.1223 || 2.3207s/step\n",
      "step:210/321 || Total Loss: 6.2545 || 2.4138s/step\n",
      "step:220/321 || Total Loss: 6.1799 || 2.3578s/step\n",
      "step:230/321 || Total Loss: 6.1896 || 3.4025s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:240/321 || Total Loss: 6.2168 || 3.1654s/step\n",
      "step:250/321 || Total Loss: 6.1849 || 3.1623s/step\n",
      "step:260/321 || Total Loss: 6.2098 || 2.3427s/step\n",
      "step:270/321 || Total Loss: 6.2159 || 1.8033s/step\n",
      "step:280/321 || Total Loss: 6.2362 || 3.8119s/step\n",
      "step:290/321 || Total Loss: 6.2860 || 2.3407s/step\n",
      "step:300/321 || Total Loss: 6.3632 || 4.2842s/step\n",
      "step:310/321 || Total Loss: 6.3597 || 2.9252s/step\n",
      "step:320/321 || Total Loss: 6.3937 || 3.9469s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.3595 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:40/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 5.1807 || 2.1636s/step\n",
      "step:10/321 || Total Loss: 7.8947 || 3.1353s/step\n",
      "step:20/321 || Total Loss: 8.2547 || 2.2207s/step\n",
      "step:30/321 || Total Loss: 8.1060 || 2.3848s/step\n",
      "step:40/321 || Total Loss: 7.7455 || 2.6870s/step\n",
      "step:50/321 || Total Loss: 7.5476 || 2.7881s/step\n",
      "step:60/321 || Total Loss: 7.3930 || 1.7903s/step\n",
      "step:70/321 || Total Loss: 7.4475 || 3.0193s/step\n",
      "step:80/321 || Total Loss: 7.5623 || 3.2684s/step\n",
      "step:90/321 || Total Loss: 7.2674 || 2.7220s/step\n",
      "step:100/321 || Total Loss: 7.1134 || 2.7360s/step\n",
      "step:110/321 || Total Loss: 7.0633 || 2.9111s/step\n",
      "step:120/321 || Total Loss: 7.0430 || 3.0423s/step\n",
      "step:130/321 || Total Loss: 7.1566 || 2.6950s/step\n",
      "step:140/321 || Total Loss: 7.1710 || 1.8714s/step\n",
      "step:150/321 || Total Loss: 7.0540 || 3.6707s/step\n",
      "step:160/321 || Total Loss: 7.1882 || 3.4986s/step\n",
      "step:170/321 || Total Loss: 7.1941 || 3.1904s/step\n",
      "step:180/321 || Total Loss: 7.1983 || 2.2056s/step\n",
      "step:190/321 || Total Loss: 7.2415 || 2.5219s/step\n",
      "step:200/321 || Total Loss: 7.2002 || 3.2965s/step\n",
      "step:210/321 || Total Loss: 7.1454 || 2.1626s/step\n",
      "step:220/321 || Total Loss: 7.0758 || 3.5416s/step\n",
      "step:230/321 || Total Loss: 7.0235 || 3.1994s/step\n",
      "step:240/321 || Total Loss: 6.9745 || 3.5126s/step\n",
      "step:250/321 || Total Loss: 6.8753 || 2.4078s/step\n",
      "step:260/321 || Total Loss: 6.9694 || 3.2004s/step\n",
      "step:270/321 || Total Loss: 7.0154 || 1.9294s/step\n",
      "step:280/321 || Total Loss: 7.0016 || 3.4815s/step\n",
      "step:290/321 || Total Loss: 7.0343 || 2.5799s/step\n",
      "step:300/321 || Total Loss: 7.0468 || 3.1984s/step\n",
      "step:310/321 || Total Loss: 7.0023 || 3.1073s/step\n",
      "step:320/321 || Total Loss: 6.9426 || 2.7410s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.9099 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:41/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 3.5083 || 2.0685s/step\n",
      "step:10/321 || Total Loss: 5.8053 || 3.5477s/step\n",
      "step:20/321 || Total Loss: 6.4194 || 2.9272s/step\n",
      "step:30/321 || Total Loss: 7.1640 || 2.6500s/step\n",
      "step:40/321 || Total Loss: 6.8952 || 2.9862s/step\n",
      "step:50/321 || Total Loss: 6.7730 || 3.6307s/step\n",
      "step:60/321 || Total Loss: 6.8701 || 2.7191s/step\n",
      "step:70/321 || Total Loss: 6.9570 || 2.7561s/step\n",
      "step:80/321 || Total Loss: 7.1287 || 2.2056s/step\n",
      "step:90/321 || Total Loss: 7.0569 || 3.4015s/step\n",
      "step:100/321 || Total Loss: 7.2113 || 3.8959s/step\n",
      "step:110/321 || Total Loss: 7.0660 || 2.3367s/step\n",
      "step:120/321 || Total Loss: 7.0839 || 3.2114s/step\n",
      "step:130/321 || Total Loss: 6.9442 || 3.4386s/step\n",
      "step:140/321 || Total Loss: 6.8780 || 2.9062s/step\n",
      "step:150/321 || Total Loss: 6.8707 || 1.8774s/step\n",
      "step:160/321 || Total Loss: 6.8592 || 2.3027s/step\n",
      "step:170/321 || Total Loss: 6.7819 || 2.2517s/step\n",
      "step:180/321 || Total Loss: 6.7252 || 3.3415s/step\n",
      "step:190/321 || Total Loss: 6.6582 || 2.1636s/step\n",
      "step:200/321 || Total Loss: 6.6044 || 3.0973s/step\n",
      "step:210/321 || Total Loss: 6.5055 || 3.4696s/step\n",
      "step:220/321 || Total Loss: 6.4293 || 2.0435s/step\n",
      "step:230/321 || Total Loss: 6.5688 || 2.2447s/step\n",
      "step:240/321 || Total Loss: 6.5736 || 2.6059s/step\n",
      "step:250/321 || Total Loss: 6.5956 || 3.0343s/step\n",
      "step:260/321 || Total Loss: 6.6477 || 2.8481s/step\n",
      "step:270/321 || Total Loss: 6.6768 || 2.8832s/step\n",
      "step:280/321 || Total Loss: 6.6638 || 3.2094s/step\n",
      "step:290/321 || Total Loss: 6.6333 || 2.5649s/step\n",
      "step:300/321 || Total Loss: 6.5542 || 2.1806s/step\n",
      "step:310/321 || Total Loss: 6.4820 || 2.7300s/step\n",
      "step:320/321 || Total Loss: 6.6656 || 3.8679s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.6493 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:42/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 5.6091 || 3.2295s/step\n",
      "step:10/321 || Total Loss: 5.0709 || 3.2584s/step\n",
      "step:20/321 || Total Loss: 5.7106 || 3.1084s/step\n",
      "step:30/321 || Total Loss: 6.6149 || 3.2955s/step\n",
      "step:40/321 || Total Loss: 6.3779 || 2.3538s/step\n",
      "step:50/321 || Total Loss: 6.1653 || 3.8078s/step\n",
      "step:60/321 || Total Loss: 6.0303 || 3.1854s/step\n",
      "step:70/321 || Total Loss: 6.1628 || 2.2026s/step\n",
      "step:80/321 || Total Loss: 6.4519 || 3.7928s/step\n",
      "step:90/321 || Total Loss: 6.5565 || 2.3177s/step\n",
      "step:100/321 || Total Loss: 6.5035 || 2.0055s/step\n",
      "step:110/321 || Total Loss: 6.4738 || 2.8481s/step\n",
      "step:120/321 || Total Loss: 6.3296 || 2.8411s/step\n",
      "step:130/321 || Total Loss: 6.4760 || 3.7038s/step\n",
      "step:140/321 || Total Loss: 6.5102 || 2.4668s/step\n",
      "step:150/321 || Total Loss: 6.4471 || 3.5686s/step\n",
      "step:160/321 || Total Loss: 6.5193 || 3.6427s/step\n",
      "step:170/321 || Total Loss: 6.4614 || 3.6578s/step\n",
      "step:180/321 || Total Loss: 6.4706 || 3.2385s/step\n",
      "step:190/321 || Total Loss: 6.4657 || 2.3287s/step\n",
      "step:200/321 || Total Loss: 6.5389 || 3.0723s/step\n",
      "step:210/321 || Total Loss: 6.6521 || 2.6620s/step\n",
      "step:220/321 || Total Loss: 6.8432 || 3.0443s/step\n",
      "step:230/321 || Total Loss: 6.8469 || 2.7481s/step\n",
      "step:240/321 || Total Loss: 6.8665 || 3.7267s/step\n",
      "step:250/321 || Total Loss: 6.8493 || 3.1473s/step\n",
      "step:260/321 || Total Loss: 6.7853 || 2.4838s/step\n",
      "step:270/321 || Total Loss: 6.8566 || 2.7000s/step\n",
      "step:280/321 || Total Loss: 6.8398 || 3.5747s/step\n",
      "step:290/321 || Total Loss: 6.8281 || 2.5639s/step\n",
      "step:300/321 || Total Loss: 6.9270 || 2.4869s/step\n",
      "step:310/321 || Total Loss: 6.9168 || 3.7978s/step\n",
      "step:320/321 || Total Loss: 6.8590 || 3.4896s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.8259 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:43/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 3.0288 || 2.3918s/step\n",
      "step:10/321 || Total Loss: 7.2300 || 3.7158s/step\n",
      "step:20/321 || Total Loss: 6.1579 || 2.4028s/step\n",
      "step:30/321 || Total Loss: 6.4863 || 1.8313s/step\n",
      "step:40/321 || Total Loss: 6.7253 || 3.1483s/step\n",
      "step:50/321 || Total Loss: 6.6864 || 2.3087s/step\n",
      "step:60/321 || Total Loss: 6.4644 || 2.3728s/step\n",
      "step:70/321 || Total Loss: 6.2657 || 3.3875s/step\n",
      "step:80/321 || Total Loss: 6.0457 || 2.7701s/step\n",
      "step:90/321 || Total Loss: 6.0304 || 2.4438s/step\n",
      "step:100/321 || Total Loss: 6.0060 || 3.1954s/step\n",
      "step:110/321 || Total Loss: 5.9184 || 4.1131s/step\n",
      "step:120/321 || Total Loss: 5.8154 || 3.8579s/step\n",
      "step:130/321 || Total Loss: 6.0278 || 3.0012s/step\n",
      "step:140/321 || Total Loss: 6.0360 || 2.2417s/step\n",
      "step:150/321 || Total Loss: 5.9974 || 2.7891s/step\n",
      "step:160/321 || Total Loss: 6.0249 || 2.4638s/step\n",
      "step:170/321 || Total Loss: 5.9900 || 2.2106s/step\n",
      "step:180/321 || Total Loss: 6.0034 || 2.9442s/step\n",
      "step:190/321 || Total Loss: 6.0373 || 3.0703s/step\n",
      "step:200/321 || Total Loss: 6.0935 || 2.1896s/step\n",
      "step:210/321 || Total Loss: 6.1714 || 3.7548s/step\n",
      "step:220/321 || Total Loss: 6.2130 || 2.5279s/step\n",
      "step:230/321 || Total Loss: 6.1534 || 2.2447s/step\n",
      "step:240/321 || Total Loss: 6.1926 || 2.1366s/step\n",
      "step:250/321 || Total Loss: 6.2029 || 2.7020s/step\n",
      "step:260/321 || Total Loss: 6.2184 || 3.3385s/step\n",
      "step:270/321 || Total Loss: 6.3146 || 3.7128s/step\n",
      "step:280/321 || Total Loss: 6.3302 || 4.2252s/step\n",
      "step:290/321 || Total Loss: 6.3651 || 2.8831s/step\n",
      "step:300/321 || Total Loss: 6.3107 || 2.7421s/step\n",
      "step:310/321 || Total Loss: 6.3209 || 2.1476s/step\n",
      "step:320/321 || Total Loss: 6.3892 || 3.3264s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.3590 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:44/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 7.1544 || 2.5879s/step\n",
      "step:10/321 || Total Loss: 9.2658 || 3.8378s/step\n",
      "step:20/321 || Total Loss: 7.0381 || 3.0513s/step\n",
      "step:30/321 || Total Loss: 5.9332 || 3.6657s/step\n",
      "step:40/321 || Total Loss: 6.0621 || 3.2004s/step\n",
      "step:50/321 || Total Loss: 6.5604 || 2.6870s/step\n",
      "step:60/321 || Total Loss: 6.1510 || 1.7843s/step\n",
      "step:70/321 || Total Loss: 6.1217 || 4.8036s/step\n",
      "step:80/321 || Total Loss: 6.4295 || 2.6350s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:90/321 || Total Loss: 6.4870 || 3.4566s/step\n",
      "step:100/321 || Total Loss: 6.3631 || 2.4628s/step\n",
      "step:110/321 || Total Loss: 6.4074 || 2.4859s/step\n",
      "step:120/321 || Total Loss: 6.3339 || 2.5989s/step\n",
      "step:130/321 || Total Loss: 6.1607 || 2.2397s/step\n",
      "step:140/321 || Total Loss: 6.2586 || 3.3495s/step\n",
      "step:150/321 || Total Loss: 6.4093 || 3.1473s/step\n",
      "step:160/321 || Total Loss: 6.4046 || 3.2524s/step\n",
      "step:170/321 || Total Loss: 6.4505 || 3.0503s/step\n",
      "step:180/321 || Total Loss: 6.4608 || 2.1956s/step\n",
      "step:190/321 || Total Loss: 6.5327 || 3.8069s/step\n",
      "step:200/321 || Total Loss: 6.4939 || 2.1155s/step\n",
      "step:210/321 || Total Loss: 6.5578 || 2.6440s/step\n",
      "step:220/321 || Total Loss: 6.5412 || 2.5309s/step\n",
      "step:230/321 || Total Loss: 6.5914 || 2.3498s/step\n",
      "step:240/321 || Total Loss: 6.5268 || 4.3432s/step\n",
      "step:250/321 || Total Loss: 6.4915 || 2.0385s/step\n",
      "step:260/321 || Total Loss: 6.5197 || 2.0015s/step\n",
      "step:270/321 || Total Loss: 6.6013 || 2.7350s/step\n",
      "step:280/321 || Total Loss: 6.5450 || 2.0125s/step\n",
      "step:290/321 || Total Loss: 6.5528 || 1.6602s/step\n",
      "step:300/321 || Total Loss: 6.5388 || 3.1433s/step\n",
      "step:310/321 || Total Loss: 6.5885 || 3.2374s/step\n",
      "step:320/321 || Total Loss: 6.5892 || 2.3658s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.5848 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:45/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 6.4611 || 3.3415s/step\n",
      "step:10/321 || Total Loss: 6.8473 || 2.3127s/step\n",
      "step:20/321 || Total Loss: 6.3874 || 2.8521s/step\n",
      "step:30/321 || Total Loss: 6.5999 || 3.0903s/step\n",
      "step:40/321 || Total Loss: 6.5778 || 2.1696s/step\n",
      "step:50/321 || Total Loss: 6.7971 || 1.9845s/step\n",
      "step:60/321 || Total Loss: 6.5944 || 2.6660s/step\n",
      "step:70/321 || Total Loss: 6.4442 || 3.1644s/step\n",
      "step:80/321 || Total Loss: 6.4983 || 2.4228s/step\n",
      "step:90/321 || Total Loss: 6.7515 || 2.8201s/step\n",
      "step:100/321 || Total Loss: 6.5640 || 3.9600s/step\n",
      "step:110/321 || Total Loss: 6.5290 || 2.0305s/step\n",
      "step:120/321 || Total Loss: 6.4939 || 2.9862s/step\n",
      "step:130/321 || Total Loss: 6.4633 || 3.2704s/step\n",
      "step:140/321 || Total Loss: 6.6001 || 3.6697s/step\n",
      "step:150/321 || Total Loss: 6.6097 || 1.8894s/step\n",
      "step:160/321 || Total Loss: 6.5804 || 2.0335s/step\n",
      "step:170/321 || Total Loss: 6.6536 || 2.8701s/step\n",
      "step:180/321 || Total Loss: 6.5904 || 2.3107s/step\n",
      "step:190/321 || Total Loss: 6.6171 || 3.3766s/step\n",
      "step:200/321 || Total Loss: 6.5793 || 4.3152s/step\n",
      "step:210/321 || Total Loss: 6.4637 || 2.4328s/step\n",
      "step:220/321 || Total Loss: 6.4835 || 3.3364s/step\n",
      "step:230/321 || Total Loss: 6.4573 || 2.4398s/step\n",
      "step:240/321 || Total Loss: 6.4926 || 3.4015s/step\n",
      "step:250/321 || Total Loss: 6.5371 || 2.9112s/step\n",
      "step:260/321 || Total Loss: 6.6282 || 3.1073s/step\n",
      "step:270/321 || Total Loss: 6.6740 || 3.0263s/step\n",
      "step:280/321 || Total Loss: 6.6920 || 3.2374s/step\n",
      "step:290/321 || Total Loss: 6.7720 || 3.3745s/step\n",
      "step:300/321 || Total Loss: 6.7362 || 1.9384s/step\n",
      "step:310/321 || Total Loss: 6.6494 || 1.9254s/step\n",
      "step:320/321 || Total Loss: 6.6066 || 1.8954s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.5940 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:46/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 12.7686 || 3.3545s/step\n",
      "step:10/321 || Total Loss: 7.9256 || 3.7398s/step\n",
      "step:20/321 || Total Loss: 7.1904 || 3.2865s/step\n",
      "step:30/321 || Total Loss: 6.4543 || 3.0823s/step\n",
      "step:40/321 || Total Loss: 6.7029 || 3.0373s/step\n",
      "step:50/321 || Total Loss: 7.0554 || 3.1704s/step\n",
      "step:60/321 || Total Loss: 6.5191 || 2.7410s/step\n",
      "step:70/321 || Total Loss: 6.7414 || 4.0370s/step\n",
      "step:80/321 || Total Loss: 6.9834 || 4.2001s/step\n",
      "step:90/321 || Total Loss: 7.0722 || 3.1193s/step\n",
      "step:100/321 || Total Loss: 6.8474 || 3.5537s/step\n",
      "step:110/321 || Total Loss: 6.5285 || 1.8184s/step\n",
      "step:120/321 || Total Loss: 6.4675 || 4.4243s/step\n",
      "step:130/321 || Total Loss: 6.3525 || 3.1734s/step\n",
      "step:140/321 || Total Loss: 6.2780 || 3.3285s/step\n",
      "step:150/321 || Total Loss: 6.2607 || 3.1403s/step\n",
      "step:160/321 || Total Loss: 6.3022 || 3.5076s/step\n",
      "step:170/321 || Total Loss: 6.3001 || 2.7561s/step\n",
      "step:180/321 || Total Loss: 6.2103 || 2.5199s/step\n",
      "step:190/321 || Total Loss: 6.1631 || 3.6467s/step\n",
      "step:200/321 || Total Loss: 6.1889 || 2.6710s/step\n",
      "step:210/321 || Total Loss: 6.2400 || 1.8023s/step\n",
      "step:220/321 || Total Loss: 6.2073 || 3.4316s/step\n",
      "step:230/321 || Total Loss: 6.1361 || 2.0926s/step\n",
      "step:240/321 || Total Loss: 6.1094 || 3.2444s/step\n",
      "step:250/321 || Total Loss: 6.0758 || 3.3655s/step\n",
      "step:260/321 || Total Loss: 6.0495 || 4.1521s/step\n",
      "step:270/321 || Total Loss: 6.0251 || 3.6206s/step\n",
      "step:280/321 || Total Loss: 5.9855 || 3.7098s/step\n",
      "step:290/321 || Total Loss: 5.9613 || 2.6370s/step\n",
      "step:300/321 || Total Loss: 5.9538 || 2.9912s/step\n",
      "step:310/321 || Total Loss: 5.9502 || 2.2607s/step\n",
      "step:320/321 || Total Loss: 5.9319 || 3.4336s/step\n",
      "Finish Training.\n",
      "Total Loss: 5.9182 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:47/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 11.8302 || 2.8711s/step\n",
      "step:10/321 || Total Loss: 6.5527 || 3.2083s/step\n",
      "step:20/321 || Total Loss: 6.5577 || 2.4108s/step\n",
      "step:30/321 || Total Loss: 6.8806 || 3.6597s/step\n",
      "step:40/321 || Total Loss: 7.4098 || 3.1594s/step\n",
      "step:50/321 || Total Loss: 7.2848 || 3.7038s/step\n",
      "step:60/321 || Total Loss: 6.9869 || 3.5396s/step\n",
      "step:70/321 || Total Loss: 6.9411 || 2.5999s/step\n",
      "step:80/321 || Total Loss: 7.0146 || 3.3575s/step\n",
      "step:90/321 || Total Loss: 7.0257 || 2.8541s/step\n",
      "step:100/321 || Total Loss: 6.9101 || 2.5019s/step\n",
      "step:110/321 || Total Loss: 6.9828 || 3.0052s/step\n",
      "step:120/321 || Total Loss: 6.8099 || 2.8422s/step\n",
      "step:130/321 || Total Loss: 6.7310 || 3.4075s/step\n",
      "step:140/321 || Total Loss: 6.9165 || 2.9962s/step\n",
      "step:150/321 || Total Loss: 6.9024 || 3.0293s/step\n",
      "step:160/321 || Total Loss: 6.8174 || 3.3525s/step\n",
      "step:170/321 || Total Loss: 6.8091 || 3.5617s/step\n",
      "step:180/321 || Total Loss: 6.7029 || 3.7398s/step\n",
      "step:190/321 || Total Loss: 6.7075 || 3.5256s/step\n",
      "step:200/321 || Total Loss: 6.7399 || 1.9404s/step\n",
      "step:210/321 || Total Loss: 6.6838 || 2.4849s/step\n",
      "step:220/321 || Total Loss: 6.6755 || 3.2334s/step\n",
      "step:230/321 || Total Loss: 6.6029 || 3.4696s/step\n",
      "step:240/321 || Total Loss: 6.5599 || 2.5309s/step\n",
      "step:250/321 || Total Loss: 6.5324 || 2.3728s/step\n",
      "step:260/321 || Total Loss: 6.5217 || 3.7148s/step\n",
      "step:270/321 || Total Loss: 6.5155 || 2.5859s/step\n",
      "step:280/321 || Total Loss: 6.5121 || 2.0685s/step\n",
      "step:290/321 || Total Loss: 6.4647 || 1.8274s/step\n",
      "step:300/321 || Total Loss: 6.3896 || 3.4075s/step\n",
      "step:310/321 || Total Loss: 6.3879 || 2.8741s/step\n",
      "step:320/321 || Total Loss: 6.4547 || 3.5887s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.4286 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:48/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 2.4989 || 2.5029s/step\n",
      "step:10/321 || Total Loss: 5.3953 || 2.8872s/step\n",
      "step:20/321 || Total Loss: 6.6354 || 2.8621s/step\n",
      "step:30/321 || Total Loss: 6.6124 || 2.0365s/step\n",
      "step:40/321 || Total Loss: 6.3054 || 2.4158s/step\n",
      "step:50/321 || Total Loss: 6.9358 || 2.8731s/step\n",
      "step:60/321 || Total Loss: 7.3815 || 2.7831s/step\n",
      "step:70/321 || Total Loss: 7.1049 || 2.2437s/step\n",
      "step:80/321 || Total Loss: 7.0677 || 2.8962s/step\n",
      "step:90/321 || Total Loss: 7.0454 || 2.1336s/step\n",
      "step:100/321 || Total Loss: 6.9365 || 2.9112s/step\n",
      "step:110/321 || Total Loss: 7.1065 || 3.2014s/step\n",
      "step:120/321 || Total Loss: 6.8680 || 3.3465s/step\n",
      "step:130/321 || Total Loss: 6.9267 || 3.5406s/step\n",
      "step:140/321 || Total Loss: 7.0058 || 3.8168s/step\n",
      "step:150/321 || Total Loss: 6.7900 || 2.0895s/step\n",
      "step:160/321 || Total Loss: 6.7019 || 2.5339s/step\n",
      "step:170/321 || Total Loss: 6.6481 || 3.5276s/step\n",
      "step:180/321 || Total Loss: 6.5929 || 3.4646s/step\n",
      "step:190/321 || Total Loss: 6.5437 || 2.6069s/step\n",
      "step:200/321 || Total Loss: 6.5678 || 3.1373s/step\n",
      "step:210/321 || Total Loss: 6.6857 || 3.4866s/step\n",
      "step:220/321 || Total Loss: 6.5948 || 2.6440s/step\n",
      "step:230/321 || Total Loss: 6.5497 || 3.3115s/step\n",
      "step:240/321 || Total Loss: 6.5777 || 2.7721s/step\n",
      "step:250/321 || Total Loss: 6.5857 || 1.9505s/step\n",
      "step:260/321 || Total Loss: 6.6208 || 3.7118s/step\n",
      "step:270/321 || Total Loss: 6.5673 || 3.0152s/step\n",
      "step:280/321 || Total Loss: 6.5792 || 3.9509s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:290/321 || Total Loss: 6.5614 || 1.6772s/step\n",
      "step:300/321 || Total Loss: 6.4993 || 3.3946s/step\n",
      "step:310/321 || Total Loss: 6.5938 || 3.0022s/step\n",
      "step:320/321 || Total Loss: 6.5770 || 3.8669s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.5652 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:49/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 5.0984 || 2.4478s/step\n",
      "step:10/321 || Total Loss: 6.3576 || 1.7943s/step\n",
      "step:20/321 || Total Loss: 5.6605 || 3.4115s/step\n",
      "step:30/321 || Total Loss: 5.3596 || 2.3748s/step\n",
      "step:40/321 || Total Loss: 5.5424 || 3.2915s/step\n",
      "step:50/321 || Total Loss: 5.8131 || 2.7541s/step\n",
      "step:60/321 || Total Loss: 6.1768 || 2.0485s/step\n",
      "step:70/321 || Total Loss: 5.9118 || 3.3095s/step\n",
      "step:80/321 || Total Loss: 5.7943 || 3.6597s/step\n",
      "step:90/321 || Total Loss: 5.8772 || 2.5299s/step\n",
      "step:100/321 || Total Loss: 5.7452 || 3.3345s/step\n",
      "step:110/321 || Total Loss: 5.7255 || 3.3505s/step\n",
      "step:120/321 || Total Loss: 5.6569 || 3.0613s/step\n",
      "step:130/321 || Total Loss: 5.7703 || 3.4506s/step\n",
      "step:140/321 || Total Loss: 5.9118 || 2.3888s/step\n",
      "step:150/321 || Total Loss: 5.9753 || 3.4205s/step\n",
      "step:160/321 || Total Loss: 5.9246 || 3.3755s/step\n",
      "step:170/321 || Total Loss: 5.9278 || 1.6983s/step\n",
      "step:180/321 || Total Loss: 5.9478 || 1.5482s/step\n",
      "step:190/321 || Total Loss: 5.9313 || 2.7440s/step\n",
      "step:200/321 || Total Loss: 5.9599 || 3.3135s/step\n",
      "step:210/321 || Total Loss: 5.9673 || 3.4426s/step\n",
      "step:220/321 || Total Loss: 5.9541 || 2.4738s/step\n",
      "step:230/321 || Total Loss: 5.9021 || 2.4919s/step\n",
      "step:240/321 || Total Loss: 5.8492 || 3.0443s/step\n",
      "step:250/321 || Total Loss: 5.8643 || 2.4899s/step\n",
      "step:260/321 || Total Loss: 5.8518 || 2.6660s/step\n",
      "step:270/321 || Total Loss: 6.0330 || 2.0455s/step\n",
      "step:280/321 || Total Loss: 6.1234 || 2.9272s/step\n",
      "step:290/321 || Total Loss: 6.1931 || 3.6378s/step\n",
      "step:300/321 || Total Loss: 6.1472 || 3.7598s/step\n",
      "step:310/321 || Total Loss: 6.3013 || 2.2547s/step\n",
      "step:320/321 || Total Loss: 6.3439 || 2.8431s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.3122 || Val Loss: 0.0000 \n",
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:50/50\n",
      "Start Training.\n",
      "step:1/321 || Total Loss: 6.8668 || 1.8984s/step\n",
      "step:10/321 || Total Loss: 6.1190 || 3.1724s/step\n",
      "step:20/321 || Total Loss: 5.6015 || 2.3818s/step\n",
      "step:30/321 || Total Loss: 5.8190 || 2.5819s/step\n",
      "step:40/321 || Total Loss: 5.6857 || 2.5439s/step\n",
      "step:50/321 || Total Loss: 5.7823 || 4.1151s/step\n",
      "step:60/321 || Total Loss: 5.7472 || 3.2995s/step\n",
      "step:70/321 || Total Loss: 5.7766 || 2.7731s/step\n",
      "step:80/321 || Total Loss: 5.5691 || 3.4826s/step\n",
      "step:90/321 || Total Loss: 5.7392 || 2.1696s/step\n",
      "step:100/321 || Total Loss: 5.8487 || 2.6880s/step\n",
      "step:110/321 || Total Loss: 5.9623 || 1.8974s/step\n",
      "step:120/321 || Total Loss: 5.9330 || 3.5857s/step\n",
      "step:130/321 || Total Loss: 5.9436 || 3.3945s/step\n",
      "step:140/321 || Total Loss: 5.9584 || 2.5259s/step\n",
      "step:150/321 || Total Loss: 6.1543 || 2.5309s/step\n",
      "step:160/321 || Total Loss: 5.9897 || 2.1606s/step\n",
      "step:170/321 || Total Loss: 5.9944 || 2.9312s/step\n",
      "step:180/321 || Total Loss: 6.1050 || 3.0983s/step\n",
      "step:190/321 || Total Loss: 6.1172 || 4.0480s/step\n",
      "step:200/321 || Total Loss: 6.1781 || 2.6891s/step\n",
      "step:210/321 || Total Loss: 6.3026 || 3.1654s/step\n",
      "step:220/321 || Total Loss: 6.3958 || 3.6267s/step\n",
      "step:230/321 || Total Loss: 6.3526 || 3.0032s/step\n",
      "step:240/321 || Total Loss: 6.3867 || 3.4886s/step\n",
      "step:250/321 || Total Loss: 6.3731 || 2.3558s/step\n",
      "step:260/321 || Total Loss: 6.4718 || 2.7370s/step\n",
      "step:270/321 || Total Loss: 6.4196 || 3.2634s/step\n",
      "step:280/321 || Total Loss: 6.3712 || 2.6880s/step\n",
      "step:290/321 || Total Loss: 6.4008 || 2.6980s/step\n",
      "step:300/321 || Total Loss: 6.3225 || 2.0075s/step\n",
      "step:310/321 || Total Loss: 6.3136 || 3.2704s/step\n",
      "step:320/321 || Total Loss: 6.3210 || 1.7423s/step\n",
      "Finish Training.\n",
      "Total Loss: 6.2931 || Val Loss: 0.0000 \n"
     ]
    }
   ],
   "source": [
    "#------------------------------------#\n",
    "#   解冻backbone后训练\n",
    "#------------------------------------#\n",
    "lr = 1e-4\n",
    "Batch_size = 4\n",
    "Freeze_Epoch = 25\n",
    "Unfreeze_Epoch = 50\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr, weight_decay=5e-4)\n",
    "if Cosine_lr:\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)\n",
    "else:\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "gen = Generator(Batch_size, train_lines, (input_shape[0], input_shape[1])).generate(mosaic = mosaic)\n",
    "gen_val = Generator(Batch_size, val_lines, (input_shape[0], input_shape[1])).generate(mosaic = False)\n",
    "                        \n",
    "epoch_size = int(max(1, num_train//Batch_size//2.5)) if mosaic else max(1, num_train//Batch_size)\n",
    "epoch_size_val = num_val//Batch_size\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for epoch in range(Freeze_Epoch, Unfreeze_Epoch):\n",
    "    total_loss, val_loss = fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, gen_val, \n",
    "                                         Unfreeze_Epoch, Cuda, optimizer, lr_scheduler)\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    with open('total_loss.csv', mode='a+') as total_loss_file:\n",
    "        total_loss_file.write(str(total_loss.item()) + '\\n')\n",
    "    #with open('val_loss.csv', mode='a+') as val_loss_file:\n",
    "    #    val_loss_file.write(str(val_loss.item() + '\\n')\n",
    "torch.save(best_model_weights, 'model_data/yolov4_maskdetect_weights1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Train one epoch.----------\n",
      "Epoch:26/50\n",
      "Start Training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\MaskDetect-YOLOv4-PyTorch-master 2\\nets\\yolo_training.py:504: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  tmp_targets = np.array(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:1/40 || Total Loss: 98.5296 || 2.5925s/step\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 1.21 GiB already allocated; 32.80 MiB free; 1.21 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-65f06646d3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreeze_Epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnfreeze_Epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     total_loss, val_loss = fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, gen_val, \n\u001b[1;32m---> 28\u001b[1;33m                                          Unfreeze_Epoch, Cuda, optimizer, lr_scheduler)\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b480f96e94c1>\u001b[0m in \u001b[0;36mfit_one_epoch\u001b[1;34m(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, genval, Epoch, cuda, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#         with torch.no_grad():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#             outputs = net(images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\MaskDetect-YOLOv4-PyTorch-master 2\\nets\\yolo4.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m#  backbone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mP5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\MaskDetect-YOLOv4-PyTorch-master 2\\nets\\CSPdarknet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\MaskDetect-YOLOv4-PyTorch-master 2\\nets\\CSPdarknet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2058\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2059\u001b[0m     )\n\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 4.00 GiB total capacity; 1.21 GiB already allocated; 32.80 MiB free; 1.21 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#------------------------------------#\n",
    "#   解冻backbone后训练\n",
    "#------------------------------------#\n",
    "lr = 1e-4\n",
    "Batch_size = 2\n",
    "Freeze_Epoch = 25\n",
    "Unfreeze_Epoch = 50\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr, weight_decay=5e-4)\n",
    "if Cosine_lr:\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)\n",
    "else:\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "gen = Generator(Batch_size, train_lines, (input_shape[0], input_shape[1])).generate(mosaic = mosaic)\n",
    "gen_val = Generator(Batch_size, val_lines, (input_shape[0], input_shape[1])).generate(mosaic = False)\n",
    "                        \n",
    "epoch_size = int(max(1, num_train//Batch_size//2.5)) if mosaic else max(1, num_train//Batch_size)\n",
    "epoch_size_val = num_val//Batch_size\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "best_loss = 55.1667\n",
    "best_model_weights = copy.deepcopy(net.state_dict())\n",
    "\n",
    "for epoch in range(Freeze_Epoch, Unfreeze_Epoch):\n",
    "    total_loss, val_loss = fit_one_epoch(net, yolo_losses, epoch, epoch_size, epoch_size_val, gen, gen_val, \n",
    "                                         Unfreeze_Epoch, Cuda, optimizer, lr_scheduler)\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    with open('total_loss.csv', mode='a+') as total_loss_file:\n",
    "        total_loss_file.write(str(total_loss.item()) + '\\n')\n",
    "    #with open('val_loss.csv', mode='a+') as val_loss_file:\n",
    "    #    val_loss_file.write(str(val_loss.item() + '\\n')\n",
    "torch.save(best_model_weights, 'model_data/yolov4_coco_pretrained_weights.pth1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
